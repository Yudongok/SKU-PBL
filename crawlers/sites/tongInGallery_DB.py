from __future__ import annotations

import json
import re
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, List, Optional, Tuple, TypedDict
from urllib.parse import urljoin

from bs4 import BeautifulSoup
from playwright.sync_api import Page, sync_playwright


# ==============================
# ì„¤ì •
# ==============================

@dataclass(frozen=True)
class Settings:
    base_url: str = "http://tongingallery.com/exhibitions"
    gallery_name: str = "í†µì¸í™”ë‘"
    open_time: str = "10:30"
    close_time: str = "18:30"
    timeout_ms: int = 60_000
    wait_ms: int = 1200
    user_agent: str = (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/115.0.0.0 Safari/537.36"
    )


SETTINGS = Settings()


# ==============================
# íƒ€ì…
# ==============================

class Exhibition(TypedDict):
    title: str
    address: str
    start_date: str
    end_date: str
    open_time: str
    close_time: str
    gallery_name: str
    author: str
    description: str
    img_url: List[str]


class _ListItem(TypedDict):
    ex: Exhibition
    detail_url: str


# ==============================
# ìœ í‹¸
# ==============================

def normalize_text(s: Optional[str]) -> str:
    return s.strip() if s else ""


def uniq_keep_order(items: List[str]) -> List[str]:
    return list(dict.fromkeys([x for x in items if x]))


def parse_single_date(part: str, base_date: Optional[datetime] = None) -> Optional[datetime]:
    if not part:
        return None

    s = re.sub(r"\s*\.\s*", ".", part.strip())

    # YYYY.MM.DD
    m = re.match(r"^(\d{4})\.(\d{1,2})\.(\d{1,2})$", s)
    if m:
        y, mth, d = map(int, m.groups())
        try:
            return datetime(year=y, month=mth, day=d)
        except ValueError:
            return None

    if base_date:
        # MM.DD
        m = re.match(r"^(\d{1,2})\.(\d{1,2})$", s)
        if m:
            mth, d = map(int, m.groups())
            try:
                return datetime(year=base_date.year, month=mth, day=d)
            except ValueError:
                return None

        # DD
        m = re.match(r"^(\d{1,2})$", s)
        if m:
            d = int(m.group(1))
            try:
                return datetime(year=base_date.year, month=base_date.month, day=d)
            except ValueError:
                return None

    return None


def parse_operating_day(operating_day: str) -> Tuple[str, str]:
    if not operating_day:
        return "", ""
    text = operating_day.strip()

    parts = re.split(r"\s*[-~â€“]\s*", text, maxsplit=1)
    if len(parts) != 2:
        return operating_day, ""

    start_dt = parse_single_date(parts[0])
    if not start_dt:
        return operating_day, ""

    end_dt = parse_single_date(parts[1], base_date=start_dt)
    if not end_dt:
        return start_dt.strftime("%Y-%m-%d"), ""

    return start_dt.strftime("%Y-%m-%d"), end_dt.strftime("%Y-%m-%d")


def _page_end_and_wait(page: Page) -> None:
    """
    ì´ë¯¸ì§€/ê°¤ëŸ¬ë¦¬ lazyload ëŒ€ì‘ìš©: Endë¡œ ìŠ¤í¬ë¡¤ í›„ ì ê¹ ëŒ€ê¸°
    """
    try:
        page.keyboard.press("End")
    except Exception:
        pass
    page.wait_for_timeout(SETTINGS.wait_ms)


# ==============================
# ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ (ON VIEW / UPCOMING ê³µí†µ)
# ==============================

def _collect_section(page: Page, header_text: str, mode: str) -> List[_ListItem]:
    """
    mode:
      - "onview": ê¸°ì¡´ ì½”ë“œì²˜ëŸ¼ header_row ê¸°ì¤€ sibling rowë¥¼ ì‚¬ìš©
      - "upcoming": inside ì»¨í…Œì´ë„ˆ ì•ˆì—ì„œ ìˆ˜ì§‘
    """
    header = page.locator("h6", has_text=header_text).first
    if not header.count():
        return []

    items: List[_ListItem] = []

    if mode == "onview":
        header_row = header.locator("xpath=ancestor::div[contains(@class,'doz_row')][1]")
        image_row = header_row.locator("xpath=following-sibling::div[contains(@class,'doz_row')][2]")
        text_row = header_row.locator("xpath=following-sibling::div[contains(@class,'doz_row')][3]")

        containers = text_row.locator("div.text-table")
        links = image_row.locator("a._fade_link")
        thumbs = image_row.locator("img.org_image")

    else:  # "upcoming"
        inside = header.locator("xpath=ancestor::div[contains(@class,'inside')][1]")
        containers = inside.locator("div.text-table")
        links = inside.locator("a._fade_link")
        thumbs = inside.locator("img.org_image")

    cnt = containers.count()
    print(f"[{header_text}] {cnt}ê°œ ë°œê²¬")

    for i in range(cnt):
        container = containers.nth(i)
        p_tags = container.locator("p")
        if p_tags.count() < 3:
            continue

        title = normalize_text(p_tags.nth(0).inner_text())
        date_text = normalize_text(p_tags.nth(1).inner_text())
        section = normalize_text(p_tags.nth(2).inner_text())

        # UPCOMING ìª½ì— ì˜ë¯¸ ì—†ëŠ” ë¸”ë¡ì´ ì„ì´ë©´ ê±¸ëŸ¬ë‚´ê¸°(ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        if mode == "upcoming":
            raw = normalize_text(container.inner_text())
            if "202" not in raw:
                continue

        href = (links.nth(i).get_attribute("href") or "") if links.count() > i else ""
        detail_url = urljoin(SETTINGS.base_url, href) if href else ""

        src = (thumbs.nth(i).get_attribute("src") or "") if thumbs.count() > i else ""
        thumb_url = urljoin(SETTINGS.base_url, src) if src else ""

        start_date, end_date = parse_operating_day(date_text)

        ex: Exhibition = {
            "title": title,
            "address": section,
            "start_date": start_date,
            "end_date": end_date,
            "open_time": SETTINGS.open_time,
            "close_time": SETTINGS.close_time,
            "gallery_name": SETTINGS.gallery_name,
            "author": "",
            "description": "",
            "img_url": [thumb_url] if thumb_url else [],
        }

        items.append({"ex": ex, "detail_url": detail_url})

    return items


# ==============================
# ìƒì„¸ ìˆ˜ì§‘
# ==============================

def _extract_description_from_html(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")

    for element in soup(["script", "style", "header", "footer", "nav", "aside"]):
        element.decompose()

    all_text = soup.get_text(separator=" |LINE| ")
    lines = all_text.split(" |LINE| ")

    garbage_keywords = [
        "í†µì¸í™”ë‘", "tong-in",
        "ê²Œì‹œë¬¼", "ëŒ“ê¸€", "ë‹µê¸€",
        "ê³µì§€", "ì•Œë ¤ì¤ë‹ˆë‹¤",
        "ë¡œê·¸ì¸", "login",
        "all right reserved", "copyright", "insadong",
    ]

    valid: List[str] = []
    for line in lines:
        clean = line.strip()
        if not clean:
            continue

        low = clean.lower()
        if any(kw in low for kw in garbage_keywords):
            continue

        # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œê±°
        if len(clean) < 15:
            continue

        # í•œê¸€ í¬í•¨ë§Œ ì±„íƒ
        if re.search(r"[ê°€-í£]", clean):
            valid.append(clean)

    return "\n".join(valid).strip()


def _extract_gallery_images(page: Page) -> List[str]:
    """
    ì‚¬ì´íŠ¸ì˜ ê°¤ëŸ¬ë¦¬ ì´ë¯¸ì§€ê°€ divì— data-src/data-bgë¡œ ë“¤ì–´ê°€ëŠ” êµ¬ì¡° ëŒ€ì‘
    """
    raw = page.evaluate(
        """() => {
            const imgs = [];
            const targets = document.querySelectorAll(
                'div._gallery_wrap ._img_wrap, div.img_wrap._img_wrap'
            );
            targets.forEach(el => {
                let src = el.getAttribute('data-src') || el.getAttribute('data-bg');
                if (src) {
                    src = src.replace(/^url\\(['"]?/, '').replace(/['"]?\\)$/, '');
                    imgs.push(src);
                }
            });
            return imgs;
        }"""
    )

    urls: List[str] = []
    for src in raw or []:
        if not src:
            continue
        s = src.strip()
        if not s:
            continue
        if s.startswith("http"):
            urls.append(s)
        else:
            urls.append(urljoin(SETTINGS.base_url, s))

    return uniq_keep_order(urls)


def _enrich_detail(page: Page, item: _ListItem) -> None:
    ex = item["ex"]
    url = item["detail_url"]
    if not url:
        return

    print(f"ğŸ‘‰ ì´ë™: {ex['title']} -> {url}")

    try:
        page.goto(url, timeout=SETTINGS.timeout_ms)
        page.wait_for_load_state("networkidle")
        _page_end_and_wait(page)
        page.wait_for_load_state("networkidle")
    except Exception as e:
        print(f"   âŒ ë¡œë”© ì—ëŸ¬: {e}")
        return

    # í…ìŠ¤íŠ¸(ì„¤ëª…)
    html = page.content()
    desc = _extract_description_from_html(html)
    ex["description"] = desc

    if desc:
        print(f"   âœ… ì„¤ëª…: {desc[:30].replace(chr(10), ' ')}...")
    else:
        print("   âš ï¸ ì„¤ëª…ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    # ì´ë¯¸ì§€
    detail_imgs = _extract_gallery_images(page)
    ex["img_url"] = uniq_keep_order((ex.get("img_url") or []) + detail_imgs)


# ==============================
# ê³µê°œ API
# ==============================

def crawl() -> List[Exhibition]:
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(user_agent=SETTINGS.user_agent)
        page = context.new_page()

        print(f"[ì ‘ì†] {SETTINGS.base_url}")
        try:
            page.goto(SETTINGS.base_url, timeout=SETTINGS.timeout_ms)
            page.wait_for_load_state("networkidle")
            page.wait_for_timeout(SETTINGS.wait_ms)
        except Exception as e:
            print(f"[ì—ëŸ¬] ì ‘ì† ì‹¤íŒ¨: {e}")
            browser.close()
            return []

        items: List[_ListItem] = []
        items += _collect_section(page, "ON VIEW", mode="onview")
        items += _collect_section(page, "UPCOMING", mode="upcoming")

        print(f"\n[ë¦¬ìŠ¤íŠ¸ ì™„ë£Œ] ì´ {len(items)}ê°œ ìˆ˜ì§‘ë¨.\n")

        for it in items:
            _enrich_detail(page, it)

        browser.close()
        return [it["ex"] for it in items]


def run(save_json: bool = True) -> List[Dict[str, Any]]:
    """
    âœ… runner.pyê°€ ì´ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë„ë¡ ë§ì¶”ëŠ” 'ì—”íŠ¸ë¦¬ í•¨ìˆ˜'
    """
    data = crawl()

    if save_json:
        json_dir = Path(__file__).resolve().parent / "json"
        json_dir.mkdir(parents=True, exist_ok=True)

        out_path = json_dir / "tongInGallery.json"
        out_path.write_text(
            json.dumps(data, ensure_ascii=False, indent=2),
            encoding="utf-8",
        )
        print(f"[JSON] ì €ì¥ ì™„ë£Œ: {out_path}")

    return data


if __name__ == "__main__":
    run(save_json=True)
