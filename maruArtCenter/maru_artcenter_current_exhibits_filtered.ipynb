{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [설치 - 필요시만 실행]\n",
        "# 주피터 커널에 패키지가 없다면 아래 주석을 풀고 한 번만 실행하세요.\n",
        "# %pip install requests beautifulsoup4 lxml pandas urllib3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [임포트 & 전역 설정]\n",
        "from __future__ import annotations\n",
        "import os, re, time\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Optional, List, Dict\n",
        "from urllib.parse import urljoin, urlparse, parse_qs, quote\n",
        "\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "from bs4 import BeautifulSoup, NavigableString, Tag\n",
        "import pandas as pd\n",
        "\n",
        "# 대상 목록 URL (마루아트센터 - 현재전시)\n",
        "LIST_URL = \"https://maruartcenter.co.kr/default/exhibit/exhibit01.php?sub=01\"\n",
        "\n",
        "# 파서 우선순위 (lxml이 설치되어 있으면 사용 권장)\n",
        "PARSER = \"lxml\"  # 설치가 안 되어 있으면 html.parser로 자동 폴백\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/124.0 Safari/537.36\"\n",
        "    ),\n",
        "    \"Accept-Language\": \"ko-KR,ko;q=0.9,en-US;q=0.8\",\n",
        "}\n",
        "TIMEOUT = 15\n",
        "SLEEP_BETWEEN = 0.3   # 요청 간 짧은 대기 (서버 예의상)\n",
        "MAX_PAGES = 1         # 목록 페이징 개수\n",
        "IMG_DIR = \"maru_images\"  # 이미지 저장 루트\n",
        "DOWNLOAD_IMAGES = False   # True로 두면 이미지 파일 다운로드\n",
        "MAX_IMGS_PER_POST = None  # None = 제한 없음, 정수 = 최대 저장 개수\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [세션/유틸 - 요청 세션 만들기]\n",
        "def get_session() -> requests.Session:\n",
        "    \"\"\"재시도/커넥션풀 설정된 세션 반환\"\"\"\n",
        "    s = requests.Session()\n",
        "    s.headers.update(HEADERS)\n",
        "    retry = Retry(\n",
        "        total=3,\n",
        "        backoff_factor=0.4,\n",
        "        status_forcelist=[429, 500, 502, 503, 504],\n",
        "        allowed_methods=[\"GET\", \"HEAD\"],\n",
        "    )\n",
        "    s.mount(\"http://\", HTTPAdapter(max_retries=retry, pool_connections=8, pool_maxsize=16))\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retry, pool_connections=8, pool_maxsize=16))\n",
        "    return s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [세션/유틸 - Soup 생성기]\n",
        "def _make_soup(html: str) -> BeautifulSoup:\n",
        "    try:\n",
        "        return BeautifulSoup(html, PARSER)\n",
        "    except Exception:\n",
        "        return BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "def _get_soup(url: str, s: requests.Session, *, referrer: Optional[str] = None) -> BeautifulSoup:\n",
        "    headers = dict(HEADERS)\n",
        "    if referrer:\n",
        "        headers[\"Referer\"] = referrer  # 핫링크/출처 검증 회피\n",
        "    r = s.get(url, headers=headers, timeout=TIMEOUT)\n",
        "    # 인코딩 보정\n",
        "    if not r.encoding or r.encoding.lower() in (\"iso-8859-1\", \"ascii\"):\n",
        "        r.encoding = r.apparent_encoding or r.encoding\n",
        "    return _make_soup(r.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [텍스트/라벨 유틸]\n",
        "def _clean_text(txt: str) -> str:\n",
        "    if not txt:\n",
        "        return \"\"\n",
        "    txt = txt.replace(\"\\xa0\", \" \")\n",
        "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
        "    return txt\n",
        "\n",
        "def _norm(s: str) -> str:\n",
        "    \"\"\"라벨 비교용 정규화: 공백/콜론 등 제거\"\"\"\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    s = s.replace(\"\\xa0\", \" \")\n",
        "    s = re.sub(r\"\\s+\", \"\", s)\n",
        "    s = s.replace(\":\", \"\").replace(\"：\", \"\")\n",
        "    return s\n",
        "\n",
        "def _abs_url(base: str, u: str) -> str:\n",
        "    return urljoin(base, u)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [목록 수집]\n",
        "def list_current_detail_urls(list_url: str = LIST_URL, *, max_pages: int = MAX_PAGES) -> List[str]:\n",
        "    \"\"\"현재전시 목록 페이지에서 상세(read_form) 링크를 수집.\"\"\"\n",
        "    out: List[str] = []\n",
        "    seen = set()\n",
        "    with get_session() as s:\n",
        "        for page in range(1, max_pages + 1):\n",
        "            url = list_url\n",
        "            if page > 1:\n",
        "                sep = '&' if '?' in url else '?'\n",
        "                url = f\"{url}{sep}com_board_page={page}\"\n",
        "            soup = _get_soup(url, s)\n",
        "            # read_form + com_board_idx가 들어간 상세 링크만 추출\n",
        "            for a in soup.select('a[href*=\"exhibit01.php\"][href*=\"read_form\"], a[href*=\"com_board_idx=\"]'):\n",
        "                href = a.get(\"href\")\n",
        "                if not href:\n",
        "                    continue\n",
        "                u = urljoin(list_url, href)\n",
        "                if \"read_form\" in u and \"com_board_idx=\" in u:\n",
        "                    if u not in seen:\n",
        "                        seen.add(u)\n",
        "                        out.append(u)\n",
        "            time.sleep(SLEEP_BETWEEN)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [패턴 정의]\n",
        "_PERIOD_PATTERNS = [\n",
        "    re.compile(r\"(?P<s>\\d{4}\\.\\s*\\d{1,2}\\.\\s*\\d{1,2})\\s*[-–~]\\s*(?P<e>\\d{1,2}\\.\\s*\\d{1,2})\"),\n",
        "    re.compile(r\"(?P<s>\\d{4}\\.\\s*\\d{1,2}\\.\\s*\\d{1,2})\\s*[-–~]\\s*(?P<e>\\d{4}\\.\\s*\\d{1,2}\\.\\s*\\d{1,2})\"),\n",
        "    re.compile(r\"기간\\s*[:：]?\\s*(?P<s>\\d{4}\\.\\s*\\d{1,2}\\.\\s*\\d{1,2})\\s*[-–~]\\s*(?P<e>\\d{1,2}\\.\\s*\\d{1,2}(?:\\.\\s*\\d{1,2})?)\"),\n",
        "]\n",
        "\n",
        "_SECTION_LABELS = [\n",
        "    (\"전시설명\", re.compile(r\"^\\s*\\[?전시\\s*설명\\]?\\s*$\")),\n",
        "    (\"전시서문\", re.compile(r\"^\\s*\\[?전시\\s*서문\\]?\\s*$\")),\n",
        "    (\"작가노트\", re.compile(r\"^\\s*\\[?작가\\s*노트\\]?\\s*$\")),\n",
        "    (\"작가의 글\", re.compile(r\"^\\s*\\[?작가의\\s*글\\]?\\s*$\")),\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [테이블 필드 추출 - 견고 버전]\n",
        "def _extract_table_field(soup: BeautifulSoup, label_keywords: List[str]) -> Optional[str]:\n",
        "    \"\"\"테이블에서 라벨(TD)과 값(TD)을 매칭해 값 추출.\"\"\"\n",
        "    keys = [_norm(k) for k in label_keywords]\n",
        "\n",
        "    # (1) 클래스가 있는 경우 우선 이용\n",
        "    for td in soup.select(\"td.board_bgcolor\"):\n",
        "        left = _clean_text(td.get_text(\" \"))\n",
        "        if _norm(left) in keys or any(k in _norm(left) for k in keys):\n",
        "            tr = td.find_parent(\"tr\")\n",
        "            if tr:\n",
        "                cand = tr.find(\"td\", class_=\"board_desc\")\n",
        "                if cand:\n",
        "                    val = _clean_text(cand.get_text(\" \"))\n",
        "                    if val:\n",
        "                        return val\n",
        "                tds = tr.find_all(\"td\")\n",
        "                try:\n",
        "                    i = tds.index(td)\n",
        "                    for j in range(i+1, len(tds)):\n",
        "                        val = _clean_text(tds[j].get_text(\" \"))\n",
        "                        if val:\n",
        "                            return val\n",
        "                except ValueError:\n",
        "                    pass\n",
        "\n",
        "    # (2) 모든 tr을 훑으며 인접 td 쌍 스캔\n",
        "    for tr in soup.select(\"tr\"):\n",
        "        tds = tr.find_all(\"td\")\n",
        "        if len(tds) < 2:\n",
        "            continue\n",
        "        for i in range(len(tds) - 1):\n",
        "            left = _clean_text(tds[i].get_text(\" \"))\n",
        "            right = _clean_text(tds[i+1].get_text(\" \"))\n",
        "            if not left and right:\n",
        "                left, right = right, left  # 좌우가 바뀐 레이아웃 대비\n",
        "            ln = _norm(left)\n",
        "            if ln in keys or any(k in ln for k in keys):\n",
        "                if right:\n",
        "                    return right\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [제목/기간 추출기]\n",
        "def _extract_title(soup: BeautifulSoup) -> Optional[str]:\n",
        "    # 표에서 '제목' 우선 추출\n",
        "    t = _extract_table_field(soup, [\"제목\", \"전시명\", \"Title\"])\n",
        "    if t:\n",
        "        return t\n",
        "    # og:title\n",
        "    m = soup.find(\"meta\", attrs={\"property\": \"og:title\"})\n",
        "    if m and m.get(\"content\"):\n",
        "        t = _clean_text(m[\"content\"]) \n",
        "        if t:\n",
        "            return t\n",
        "    # 헤딩/클래스 후보\n",
        "    candidates = []\n",
        "    for sel in [\"h1\", \"h2\", \".tit\", \".title\", \".subject\", \".board_tit\", \".view_tit\"]:\n",
        "        for el in soup.select(sel):\n",
        "            txt = _clean_text(el.get_text(\" \"))\n",
        "            if txt:\n",
        "                candidates.append((len(txt), txt))\n",
        "    if candidates:\n",
        "        candidates.sort(reverse=True)\n",
        "        return candidates[0][1]\n",
        "    # <title> 폴백\n",
        "    if soup.title and soup.title.string:\n",
        "        return _clean_text(soup.title.string)\n",
        "    return None\n",
        "\n",
        "def _extract_period_from_table_or_text(soup: BeautifulSoup) -> Optional[str]:\n",
        "    # 표에서 '기간' 우선 추출\n",
        "    p = _extract_table_field(soup, [\"기간\", \"전시기간\", \"전시 일정\", \"DATE\"])\n",
        "    if p:\n",
        "        return p\n",
        "    # 본문 텍스트에서 패턴 탐지\n",
        "    text = _clean_text(soup.get_text(\"\\n\"))\n",
        "    for pat in _PERIOD_PATTERNS:\n",
        "        m = pat.search(text)\n",
        "        if m:\n",
        "            s = _clean_text(m.group(\"s\"))\n",
        "            e = _clean_text(m.group(\"e\"))\n",
        "            return f\"{s} - {e}\"\n",
        "    # 라인에 '기간' 포함 시 그 라인 반환\n",
        "    for line in text.splitlines():\n",
        "        if \"기간\" in line:\n",
        "            line = _clean_text(line)\n",
        "            if len(line) > 3:\n",
        "                return line\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "75585724",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [본문 스코프 & 푸터 필터 헬퍼]\n",
        "\n",
        "# 본문 컨테이너 후보 (페이지에 따라 다를 수 있어 넓게 지정)\n",
        "_CONTENT_SELECTORS = \".board_view, .view, .view_area, .view_con, .board, #board, .content, .editor, .viewDetail\"\n",
        "\n",
        "# 푸터/하단 영역 셀렉터 (제외)\n",
        "_FOOTER_SELECTORS = \"footer, #footer, .footer, .foot, .ft, .bottom, .site-info, address\"\n",
        "\n",
        "# 푸터 키워드/패턴 (라인 단위로 감지)\n",
        "_FOOTER_PAT = re.compile(\n",
        "    r\"(\"\n",
        "    r\"개인정보(처리|취급)방침|이메일무단수집거부|오시는길|고객센터|회사\\s*:|대표자|사업자|사업자등록|\"\n",
        "    r\"주소\\s*:|Tel\\s*:|Fax\\s*:|EMAIL\\s*:|E-?mail\\s*:|Copyright|COPYRIGHT|All\\s+Right[s]?\\s+Reserved\"\n",
        "    r\")\",\n",
        "    re.IGNORECASE\n",
        ")\n",
        "\n",
        "def get_content_scope(soup: BeautifulSoup) -> Tag:\n",
        "    \"\"\"\n",
        "    본문 컨테이너를 찾아 반환.\n",
        "    - 후보 셀렉터 중 첫 번째를 사용\n",
        "    - 찾지 못하면 문서 전체 반환하되, 이후 푸터 영역은 제거\n",
        "    \"\"\"\n",
        "    scope = soup.select_one(_CONTENT_SELECTORS) or soup\n",
        "    # scope 내부에서 푸터/하단 영역은 제거(있으면)\n",
        "    for f in scope.select(_FOOTER_SELECTORS):\n",
        "        f.decompose()\n",
        "    return scope\n",
        "\n",
        "def is_footer_text(text: str) -> bool:\n",
        "    \"\"\"한 줄이 푸터/연락처/저작권 안내에 해당하는지\"\"\"\n",
        "    t = _clean_text(text)\n",
        "    if not t:\n",
        "        return False\n",
        "    return bool(_FOOTER_PAT.search(t))\n",
        "\n",
        "def trim_footer_tail(block_text: str) -> str:\n",
        "    \"\"\"\n",
        "    블록 텍스트의 '끝부분'에서 푸터 패턴이 보이면 그 지점부터 잘라냄.\n",
        "    (본문 끝에 붙은 연락처/카피라이트 꼬리를 제거)\n",
        "    \"\"\"\n",
        "    if not block_text:\n",
        "        return block_text\n",
        "    lines = [l.rstrip() for l in block_text.splitlines()]\n",
        "    cut = len(lines)\n",
        "    for i, line in enumerate(lines):\n",
        "        if is_footer_text(line):\n",
        "            cut = i\n",
        "            break\n",
        "    cleaned = \"\\n\".join(lines[:cut]).rstrip()\n",
        "    # 과도한 빈줄 정리\n",
        "    cleaned = re.sub(r\"\\n{3,}\", \"\\n\\n\", cleaned)\n",
        "    return cleaned\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [섹션 추출 헬퍼]\n",
        "\n",
        "# 라벨 판별(대괄호 유/무 모두 허용)\n",
        "_LABEL_RE = re.compile(r\"^\\s*\\[?(전시\\s*설명|전시\\s*서문|작가\\s*노트|작가의\\s*글)\\]?\\s*$\")\n",
        "\n",
        "def _is_label_text(t: str) -> bool:\n",
        "    return bool(_LABEL_RE.match(_clean_text(t)))\n",
        "\n",
        "def _nearest_block(tag: Tag) -> Tag:\n",
        "    cur = tag\n",
        "    while cur and isinstance(cur, Tag) and cur.name.lower() not in {\"p\", \"div\", \"li\", \"section\", \"article\"}:\n",
        "        cur = cur.parent\n",
        "    return cur if isinstance(cur, Tag) else tag\n",
        "\n",
        "def _collect_following_text(start: Tag) -> str:\n",
        "    \"\"\"\n",
        "    라벨 블록 이후를 '문서 순회'로 수집.\n",
        "    - 형제만 보지 않고 .find_all_next()로 다음 요소들을 따라가며 모음\n",
        "    - 다음 섹션 경계(헤딩/새 라벨/푸터/수평선/분리선 라인)에서 중지\n",
        "    - <br> 줄바꿈 보존, <strong>/<b>는 본문 허용\n",
        "    \"\"\"\n",
        "    buf: List[str] = []\n",
        "    started = False\n",
        "\n",
        "    # 현재 블록 포함 이후를 순서대로 훑는다\n",
        "    for el in start.find_all_next():\n",
        "        # start 자신을 지나친 뒤부터 수집 시작\n",
        "        if el is start:\n",
        "            started = True\n",
        "            continue\n",
        "        if not started:\n",
        "            continue\n",
        "\n",
        "        # 푸터 영역이면 중지\n",
        "        if isinstance(el, Tag):\n",
        "            if el.name.lower() in {\"footer\", \"address\"}:\n",
        "                break\n",
        "            if el.select_one(_FOOTER_SELECTORS):\n",
        "                break\n",
        "            # 섹션 헤딩(h1~h4)이면 다음 섹션 시작으로 간주\n",
        "            if el.name.lower() in {\"h1\", \"h2\", \"h3\", \"h4\"}:\n",
        "                break\n",
        "            # 수평선으로도 절단\n",
        "            if el.name.lower() in {\"hr\"}:\n",
        "                break\n",
        "\n",
        "        # 텍스트 추출\n",
        "        t = \"\"\n",
        "        if isinstance(el, NavigableString):\n",
        "            t = _clean_text(str(el))\n",
        "        elif isinstance(el, Tag):\n",
        "            # 글자 없는 컨테이너는 스킵 (이미 푸터/헤딩은 위에서 걸렀음)\n",
        "            t = _clean_text(el.get_text(\"\\n\"))\n",
        "\n",
        "        if not t:\n",
        "            continue\n",
        "\n",
        "        # 새 라벨 패턴을 만나면 종료\n",
        "        if _is_label_text(t):\n",
        "            break\n",
        "\n",
        "        # 흔한 분리선(—, -, ▼, ▲ 등)을 만나면 종료\n",
        "        if t.strip() in {\"-\", \"–\", \"—\", \"▼\", \"▲\"}:\n",
        "            break\n",
        "\n",
        "        buf.append(t)\n",
        "\n",
        "        # 과다 수집 방지\n",
        "        if sum(len(x) for x in buf) > 12000:\n",
        "            break\n",
        "\n",
        "    out = \"\\n\".join(x for x in buf if x).strip()\n",
        "    out = re.sub(r\"\\n{3,}\", \"\\n\\n\", out)\n",
        "    out = trim_footer_tail(out)  # 본문 끝의 푸터 꼬리 제거\n",
        "    return out\n",
        "\n",
        "\n",
        "def _extract_sections(soup: BeautifulSoup) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    전시설명/전시서문/작가노트/작가의 글 텍스트 추출\n",
        "    - 문서 전체가 아닌 본문 스코프(get_content_scope)에서만 탐색\n",
        "    - 푸터 영역은 사전 제거\n",
        "    \"\"\"\n",
        "    root = get_content_scope(soup)\n",
        "    textmap: Dict[str, str] = {}\n",
        "\n",
        "    # (1) [라벨] 패턴 먼저\n",
        "    for node in root.find_all(string=True):\n",
        "        s = _clean_text(str(node))\n",
        "        if not s:\n",
        "            continue\n",
        "        if _is_label_text(s):\n",
        "            block = _nearest_block(node if isinstance(node, Tag) else node.parent)\n",
        "            content = _collect_following_text(block)\n",
        "            if content:\n",
        "                label = _clean_text(s).strip(\"[]\")\n",
        "                textmap[label] = content\n",
        "\n",
        "    # (2) 대괄호 없이 헤딩/볼드로만 라벨이 있는 경우\n",
        "    if not textmap:\n",
        "        for lab, pat in _SECTION_LABELS:\n",
        "            for el in root.find_all([\"h1\", \"h2\", \"h3\", \"strong\", \"b\", \"p\", \"div\"]):\n",
        "                t = _clean_text(el.get_text(\" \"))\n",
        "                if pat.match(t):\n",
        "                    content = _collect_following_text(el)\n",
        "                    if content:\n",
        "                        textmap[lab] = content\n",
        "\n",
        "    # (3) 폴백: 본문에서 가장 긴 문단 (푸터 꼬리 제거)\n",
        "    if not textmap:\n",
        "        paragraphs = [\n",
        "            _clean_text(p.get_text(\"\\n\"))\n",
        "            for p in root.find_all([\"p\", \"div\"])\n",
        "        ]\n",
        "        paragraphs = [p for p in paragraphs if p and len(p) >= 40]\n",
        "        paragraphs.sort(key=len, reverse=True)\n",
        "        if paragraphs:\n",
        "            textmap[\"본문\"] = trim_footer_tail(paragraphs[0])\n",
        "\n",
        "    return textmap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [이미지 수집 & 다운로드]\n",
        "_IMG_EXT = (\".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\", \".avif\")\n",
        "\n",
        "# 불필요 이미지(아이콘/로고/네비게이션 버튼 등) 필터 규칙\n",
        "_EXCLUDE_PATH_SUBSTR = [\n",
        "    \"/default/img/common/\",          # 공통 아이콘/로고 경로\n",
        "    \"/img/common/\",                  # 공통 이미지 경로\n",
        "    \"/component/board/board_10/list.gif\",\n",
        "    \"/component/board/board_10/write.gif\",\n",
        "]\n",
        "_EXCLUDE_NAME_EXACT = {\n",
        "    \"icon-phone.png\", \"icon-insta.png\", \"icon-blog.png\", \"icon-map.png\",\n",
        "    \"icon-top.png\", \"logo.png\", \"logo-m.png\", \"logo-f.png\",\n",
        "}\n",
        "_EXCLUDE_NAME_PREFIX = (\"icon\", \"logo\")  # 파일명이 icon*, logo* 로 시작하면 제외\n",
        "\n",
        "def _norm_name_from_url(u: str) -> str:\n",
        "    name = os.path.basename(urlparse(u).path)\n",
        "    name = re.sub(r'^thumb-', '', name, flags=re.IGNORECASE)\n",
        "    name = re.sub(r'_(\\d+)x(\\d+)(?=\\.[A-Za-z0-9]+$)', '', name)\n",
        "    return name.lower()\n",
        "\n",
        "def dedupe_img_urls_by_key(img_urls: List[str]) -> List[str]:\n",
        "    uniq, seen = [], set()\n",
        "    for u in img_urls:\n",
        "        key = _norm_name_from_url(u)\n",
        "        if key and key not in seen:\n",
        "            seen.add(key)\n",
        "            uniq.append(u)\n",
        "    return uniq\n",
        "\n",
        "def _should_keep_image(u: str) -> bool:\n",
        "    \"\"\"아이콘/로고/네비게이션 버튼 등 불필요 이미지를 제외\"\"\"\n",
        "    p = urlparse(u).path.lower()\n",
        "    name = os.path.basename(p)\n",
        "    # 경로 기반 제외\n",
        "    for sub in _EXCLUDE_PATH_SUBSTR:\n",
        "        if sub in p:\n",
        "            return False\n",
        "    # 파일명 정확 매칭 제외\n",
        "    if name in _EXCLUDE_NAME_EXACT:\n",
        "        return False\n",
        "    # 접두사 기반 제외\n",
        "    if name.startswith(_EXCLUDE_NAME_PREFIX):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def collect_image_urls(detail_url: str, soup: BeautifulSoup) -> List[str]:\n",
        "    \"\"\"상세 본문에서 이미지 URL을 절대경로로 수집 (lazy-src 포함)\"\"\"\n",
        "    urls: List[str] = []\n",
        "    scope = soup.select_one(\".board_view, .view, .view_area, .view_con, .board, #board, .content, .editor\") or soup\n",
        "    for img in scope.find_all(\"img\"):\n",
        "        cand = None\n",
        "        for attr in (\"src\", \"data-src\", \"data-original\", \"data-lazy\", \"data-echo\"):\n",
        "            v = img.get(attr)\n",
        "            if v and isinstance(v, str):\n",
        "                cand = v\n",
        "                break\n",
        "        if not cand:\n",
        "            continue\n",
        "        u = _abs_url(detail_url, cand)\n",
        "        # 확장자 필터(없어도 수집, 다만 확장자 있으면 체크) + 불필요 이미지 제외\n",
        "        path = urlparse(u).path.lower()\n",
        "        if ((not os.path.splitext(path)[1]) or path.endswith(_IMG_EXT)) and _should_keep_image(u):\n",
        "            urls.append(u)\n",
        "    # 중복 제거\n",
        "    urls = dedupe_img_urls_by_key(urls)\n",
        "    return urls\n",
        "\n",
        "def _filename_from_url_or_headers(url: str, resp) -> str:\n",
        "    base = os.path.basename(urlparse(url).path)\n",
        "    if base:\n",
        "        return base\n",
        "    cd = resp.headers.get(\"Content-Disposition\", \"\")\n",
        "    m = re.search(r'filename\\*?=(?:UTF-8\\'\\')?\"?([^\";]+)\"?', cd)\n",
        "    if m:\n",
        "        return m.group(1)\n",
        "    ctype = (resp.headers.get(\"Content-Type\") or \"\").lower()\n",
        "    if \"png\" in ctype: return \"image.png\"\n",
        "    if \"webp\" in ctype: return \"image.webp\"\n",
        "    return \"image.jpg\"\n",
        "\n",
        "def download_images_from_urls(detail_url: str, img_urls: List[str], img_dir: str = IMG_DIR, max_imgs: Optional[int] = MAX_IMGS_PER_POST) -> List[str]:\n",
        "    if not img_urls:\n",
        "        return []\n",
        "    img_urls = dedupe_img_urls_by_key(img_urls)\n",
        "    qs = parse_qs(urlparse(detail_url).query)\n",
        "    post_id = qs.get(\"com_board_idx\", [\"unknown\"])[0]\n",
        "    subdir = os.path.join(img_dir, re.sub(r\"[^0-9A-Za-z_-]\", \"_\", post_id))\n",
        "    os.makedirs(subdir, exist_ok=True)\n",
        "\n",
        "    saved: List[str] = []\n",
        "    tried = 0\n",
        "    with get_session() as s:\n",
        "        _ = _get_soup(detail_url, s)  # 프리히트\n",
        "        for u in img_urls:\n",
        "            if max_imgs is not None and tried >= max_imgs:\n",
        "                break\n",
        "            tried += 1\n",
        "            try:\n",
        "                r = s.get(\n",
        "                    u,\n",
        "                    headers={**HEADERS, \"Referer\": detail_url, \"Accept\": \"image/avif,image/webp,image/apng,image/*,*/*;q=0.8\"},\n",
        "                    timeout=TIMEOUT,\n",
        "                    allow_redirects=True,\n",
        "                )\n",
        "                ctype = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
        "                if r.status_code == 200 and r.content and \"image\" in ctype:\n",
        "                    name = _filename_from_url_or_headers(u, r)\n",
        "                    if \".\" not in os.path.basename(name):\n",
        "                        if \"png\" in ctype: name += \".png\"\n",
        "                        elif \"webp\" in ctype: name += \".webp\"\n",
        "                        else: name += \".jpg\"\n",
        "                    base, ext = os.path.splitext(name)\n",
        "                    final = os.path.join(subdir, name)\n",
        "                    k = 1\n",
        "                    while os.path.exists(final):\n",
        "                        final = os.path.join(subdir, f\"{base}_{k}{ext}\")\n",
        "                        k += 1\n",
        "                    with open(final, \"wb\") as f:\n",
        "                        f.write(r.content)\n",
        "                    saved.append(final)\n",
        "                else:\n",
        "                    print(f\"[이미지 응답 이상] {r.status_code} {u} (ctype={ctype})\")\n",
        "            except Exception as e:\n",
        "                print(f\"[이미지 실패] {u} -> {e}\")\n",
        "    return saved\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [데이터 모델 & 상세 파서]\n",
        "@dataclass\n",
        "class ExhibitRecord:\n",
        "    url: str\n",
        "    title: str\n",
        "    period: str\n",
        "    section_type: str\n",
        "    section_text: str\n",
        "    image_urls: List[str]\n",
        "    saved_images: List[str]\n",
        "\n",
        "def parse_detail(url: str, s: Optional[requests.Session] = None, *, download_images: bool = DOWNLOAD_IMAGES) -> ExhibitRecord:\n",
        "    own = False\n",
        "    if s is None:\n",
        "        s = get_session()\n",
        "        own = True\n",
        "    try:\n",
        "        soup = _get_soup(url, s, referrer=LIST_URL)\n",
        "        title = _extract_title(soup) or \"\"\n",
        "        period = _extract_period_from_table_or_text(soup) or \"\"\n",
        "        sections = _extract_sections(soup)\n",
        "        order = [\"전시설명\", \"전시서문\", \"작가노트\", \"작가의 글\", \"본문\"]\n",
        "        section_type, section_text = \"\", \"\"\n",
        "        for k in order:\n",
        "            if k in sections and sections[k]:\n",
        "                section_type, section_text = k, sections[k]\n",
        "                break\n",
        "        image_urls = collect_image_urls(url, soup)\n",
        "        saved = download_images_from_urls(url, image_urls) if download_images else []\n",
        "        return ExhibitRecord(\n",
        "            url=url,\n",
        "            title=title,\n",
        "            period=period,\n",
        "            section_type=section_type,\n",
        "            section_text=section_text,\n",
        "            image_urls=image_urls,\n",
        "            saved_images=saved,\n",
        "        )\n",
        "    finally:\n",
        "        if own:\n",
        "            s.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [엔드투엔드 크롤러]\n",
        "def crawl_maru_current(list_url: str = LIST_URL, *, max_pages: int = MAX_PAGES, limit: Optional[int] = None, download_images: bool = DOWNLOAD_IMAGES) -> List[ExhibitRecord]:\n",
        "    detail_urls = list_current_detail_urls(list_url, max_pages=max_pages)\n",
        "    if limit is not None:\n",
        "        detail_urls = detail_urls[:limit]\n",
        "    results: List[ExhibitRecord] = []\n",
        "    with get_session() as s:\n",
        "        _ = _get_soup(list_url, s)\n",
        "        for du in detail_urls:\n",
        "            try:\n",
        "                rec = parse_detail(du, s, download_images=download_images)\n",
        "                results.append(rec)\n",
        "            except Exception as e:\n",
        "                results.append(ExhibitRecord(\n",
        "                    url=du, title=\"\", period=\"\", section_type=\"\", section_text=f\"[ERROR] {e}\", image_urls=[], saved_images=[]\n",
        "                ))\n",
        "            time.sleep(SLEEP_BETWEEN)\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>period</th>\n",
              "      <th>section_type</th>\n",
              "      <th>images_count</th>\n",
              "      <th>first_image</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>제70회 창작미술협회전</td>\n",
              "      <td>2025.10.15-10.20</td>\n",
              "      <td></td>\n",
              "      <td>5</td>\n",
              "      <td>https://maruartcenter.co.kr/bizdemo133414/comp...</td>\n",
              "      <td>https://maruartcenter.co.kr/default/exhibit/ex...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>만화, 4·3과 민주주의를 그리다展</td>\n",
              "      <td>2025.10.15-10.20</td>\n",
              "      <td>본문</td>\n",
              "      <td>4</td>\n",
              "      <td>https://maruartcenter.co.kr/bizdemo133414/comp...</td>\n",
              "      <td>https://maruartcenter.co.kr/default/exhibit/ex...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>전경애展</td>\n",
              "      <td>2025.10.15-10.20</td>\n",
              "      <td>본문</td>\n",
              "      <td>3</td>\n",
              "      <td>https://maruartcenter.co.kr/bizdemo133414/comp...</td>\n",
              "      <td>https://maruartcenter.co.kr/default/exhibit/ex...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>제51회 영토회 領土會</td>\n",
              "      <td>2025.10.15-10.20</td>\n",
              "      <td>본문</td>\n",
              "      <td>4</td>\n",
              "      <td>https://maruartcenter.co.kr/bizdemo133414/comp...</td>\n",
              "      <td>https://maruartcenter.co.kr/default/exhibit/ex...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>예원예술대학교 만화게임영상학과 졸업전시ㅣGAGAMO GALLERY</td>\n",
              "      <td>2025.10.15-10.20</td>\n",
              "      <td>본문</td>\n",
              "      <td>4</td>\n",
              "      <td>https://maruartcenter.co.kr/bizdemo133414/comp...</td>\n",
              "      <td>https://maruartcenter.co.kr/default/exhibit/ex...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>아주 아틀리에 전시ㅣ회심</td>\n",
              "      <td>2025.10.15-10.20</td>\n",
              "      <td></td>\n",
              "      <td>3</td>\n",
              "      <td>https://maruartcenter.co.kr/bizdemo133414/comp...</td>\n",
              "      <td>https://maruartcenter.co.kr/default/exhibit/ex...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  title            period section_type  \\\n",
              "0                          제70회 창작미술협회전  2025.10.15-10.20                \n",
              "1                   만화, 4·3과 민주주의를 그리다展  2025.10.15-10.20           본문   \n",
              "2                                  전경애展  2025.10.15-10.20           본문   \n",
              "3                          제51회 영토회 領土會  2025.10.15-10.20           본문   \n",
              "4  예원예술대학교 만화게임영상학과 졸업전시ㅣGAGAMO GALLERY  2025.10.15-10.20           본문   \n",
              "5                         아주 아틀리에 전시ㅣ회심  2025.10.15-10.20                \n",
              "\n",
              "   images_count                                        first_image  \\\n",
              "0             5  https://maruartcenter.co.kr/bizdemo133414/comp...   \n",
              "1             4  https://maruartcenter.co.kr/bizdemo133414/comp...   \n",
              "2             3  https://maruartcenter.co.kr/bizdemo133414/comp...   \n",
              "3             4  https://maruartcenter.co.kr/bizdemo133414/comp...   \n",
              "4             4  https://maruartcenter.co.kr/bizdemo133414/comp...   \n",
              "5             3  https://maruartcenter.co.kr/bizdemo133414/comp...   \n",
              "\n",
              "                                                 url  \n",
              "0  https://maruartcenter.co.kr/default/exhibit/ex...  \n",
              "1  https://maruartcenter.co.kr/default/exhibit/ex...  \n",
              "2  https://maruartcenter.co.kr/default/exhibit/ex...  \n",
              "3  https://maruartcenter.co.kr/default/exhibit/ex...  \n",
              "4  https://maruartcenter.co.kr/default/exhibit/ex...  \n",
              "5  https://maruartcenter.co.kr/default/exhibit/ex...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved maru_current_exhibits.(csv|json)\n"
          ]
        }
      ],
      "source": [
        "# %% [실행 예시]\n",
        "# 필요시 설정 변경 후 실행\n",
        "DOWNLOAD_IMAGES = False  # True로 바꾸면 이미지 저장\n",
        "MAX_PAGES = 1\n",
        "\n",
        "records = crawl_maru_current(LIST_URL, max_pages=MAX_PAGES, limit=10, download_images=DOWNLOAD_IMAGES)\n",
        "df = pd.DataFrame([{**asdict(r),\n",
        "                    \"images_count\": len(r.image_urls),\n",
        "                    \"first_image\": r.image_urls[0] if r.image_urls else \"\",\n",
        "                    \"saved_count\": len(r.saved_images)} for r in records])\n",
        "\n",
        "# 주요 컬럼 미리보기\n",
        "cols = [\"title\", \"period\", \"section_type\", \"images_count\", \"first_image\", \"url\"]\n",
        "display(df[cols])\n",
        "\n",
        "# 저장(선택)\n",
        "df.to_csv(\"maru_current_exhibits.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "df.to_json(\"maru_current_exhibits.json\", orient=\"records\", force_ascii=False)\n",
        "print(\"Saved maru_current_exhibits.(csv|json)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG]  https://maruartcenter.co.kr/default/exhibit/exhibit01.php?com_board_basic=read_form&com_board_idx=769&sub=01&&com_board_search_code=&com_board_search_value1=&com_board_search_value2=&com_board_page=&&com_board_id=10&&com_board_id=10\n",
            "['제목 제70회 창작미술협회전 기간 2025.10.15-10.20 제70회 창작미술협회전 ▼ 2025.10.15 - 10.20 마루아트센터 신관 B1층 특별관', '제목', '제70회 창작미술협회전', '기간', '2025.10.15-10.20', '제70회 창작미술협회전 ▼ 2025.10.15 - 10.20 마루아트센터 신관 B1층 특별관', '', '', '', '']\n",
            "['제목', '제70회 창작미술협회전']\n",
            "['기간', '2025.10.15-10.20']\n",
            "['제70회 창작미술협회전 ▼ 2025.10.15 - 10.20 마루아트센터 신관 B1층 특별관']\n",
            "['']\n",
            "['', '', '']\n",
            "['']\n"
          ]
        }
      ],
      "source": [
        "# %% [디버그 도우미 - 테이블 구조 확인]\n",
        "# 한 개 상세글의 TR별 TD 텍스트를 확인해 라벨/값 구조를 눈으로 점검할 수 있습니다.\n",
        "if 'records' in globals() and records:\n",
        "    test_url = records[0].url\n",
        "    with get_session() as _s:\n",
        "        sp = _get_soup(test_url, _s, referrer=LIST_URL)\n",
        "        print(\"[DEBUG] \", test_url)\n",
        "        for tr in sp.select(\"tr\")[:20]:\n",
        "            cells = [_clean_text(td.get_text(\" \")) for td in tr.find_all(\"td\")]\n",
        "            if cells:\n",
        "                print(cells)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37bd4fa0",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
