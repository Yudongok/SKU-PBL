{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [설치 - 필요시만 실행]\n",
        "# %pip install requests beautifulsoup4 lxml pandas urllib3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [임포트 & 전역 설정]\n",
        "from __future__ import annotations\n",
        "import os, re, time\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Optional, List, Dict, Tuple\n",
        "from urllib.parse import urljoin, urlparse, parse_qs, quote\n",
        "\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "from bs4 import BeautifulSoup, NavigableString, Tag, Comment\n",
        "import pandas as pd\n",
        "\n",
        "LIST_URL = \"https://maruartcenter.co.kr/default/exhibit/exhibit01.php?sub=01\"\n",
        "PARSER = \"lxml\"\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/124.0 Safari/537.36\"\n",
        "    ),\n",
        "    \"Accept-Language\": \"ko-KR,ko;q=0.9,en-US;q=0.8\",\n",
        "}\n",
        "TIMEOUT = 15\n",
        "SLEEP_BETWEEN = 0.4\n",
        "MAX_PAGES = 1\n",
        "IMG_DIR = \"maru_images\"\n",
        "DOWNLOAD_IMAGES = False\n",
        "MAX_IMGS_PER_POST = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [세션/유틸 함수]\n",
        "def get_session() -> requests.Session:\n",
        "    s = requests.Session()\n",
        "    s.headers.update(HEADERS)\n",
        "    retry = Retry(\n",
        "        total=3,\n",
        "        backoff_factor=0.4,\n",
        "        status_forcelist=[429, 500, 502, 503, 504],\n",
        "        allowed_methods=[\"GET\", \"HEAD\"]\n",
        "    )\n",
        "    s.mount(\"http://\", HTTPAdapter(max_retries=retry, pool_connections=8, pool_maxsize=16))\n",
        "    s.mount(\"https://\", HTTPAdapter(max_retries=retry, pool_connections=8, pool_maxsize=16))\n",
        "    return s\n",
        "\n",
        "def _make_soup(html: str) -> BeautifulSoup:\n",
        "    try:\n",
        "        return BeautifulSoup(html, PARSER)\n",
        "    except Exception:\n",
        "        return BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "def _get_soup(url: str, s: requests.Session, *, referrer: Optional[str] = None) -> BeautifulSoup:\n",
        "    headers = dict(HEADERS)\n",
        "    if referrer:\n",
        "        headers[\"Referer\"] = referrer\n",
        "    r = s.get(url, headers=headers, timeout=TIMEOUT)\n",
        "    if not r.encoding or r.encoding.lower() in (\"iso-8859-1\", \"ascii\"):\n",
        "        r.encoding = r.apparent_encoding or r.encoding\n",
        "    return _make_soup(r.text)\n",
        "\n",
        "def _clean_text(txt: str) -> str:\n",
        "    if not txt:\n",
        "        return \"\"\n",
        "    txt = txt.replace(\"\\xa0\", \" \")\n",
        "    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
        "    return txt\n",
        "\n",
        "def _abs_url(base: str, u: str) -> str:\n",
        "    return urljoin(base, u)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [목록 → 상세 링크 수집]\n",
        "def list_current_detail_urls(list_url: str = LIST_URL, *, max_pages: int = MAX_PAGES) -> List[str]:\n",
        "    out: List[str] = []\n",
        "    seen = set()\n",
        "    with get_session() as s:\n",
        "        for page in range(1, max_pages + 1):\n",
        "            url = list_url\n",
        "            if page > 1:\n",
        "                sep = '&' if '?' in url else '?'\n",
        "                url = f\"{url}{sep}com_board_page={page}\"\n",
        "            soup = _get_soup(url, s)\n",
        "            for a in soup.select('a[href*=\"exhibit01.php\"][href*=\"read_form\"], a[href*=\"com_board_idx=\"]'):\n",
        "                href = a.get(\"href\")\n",
        "                if not href:\n",
        "                    continue\n",
        "                u = urljoin(list_url, href)\n",
        "                if \"read_form\" in u and \"com_board_idx=\" in u:\n",
        "                    if u not in seen:\n",
        "                        seen.add(u)\n",
        "                        out.append(u)\n",
        "            time.sleep(SLEEP_BETWEEN)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [패턴 정의 & 테이블 필드 추출]\n",
        "_PERIOD_PATTERNS = [\n",
        "    re.compile(r\"(?P<s>\\d{4}\\.\\s*\\d{1,2}\\.\\s*\\d{1,2})\\s*[-–~]\\s*(?P<e>\\d{1,2}\\.\\s*\\d{1,2})\"),\n",
        "    re.compile(r\"(?P<s>\\d{4}\\.\\s*\\d{1,2}\\.\\s*\\d{1,2})\\s*[-–~]\\s*(?P<e>\\d{4}\\.\\s*\\d{1,2}\\.\\s*\\d{1,2})\"),\n",
        "    re.compile(r\"기간\\s*[:：]?\\s*(?P<s>\\d{4}\\.\\s*\\d{1,2}\\.\\s*\\d{1,2})\\s*[-–~]\\s*(?P<e>\\d{1,2}\\.\\s*\\d{1,2}(?:\\.\\s*\\d{1,2})?)\"),\n",
        "]\n",
        "\n",
        "_SECTION_LABELS = [\n",
        "    (\"전시설명\", re.compile(r\"^\\s*\\[?전시\\s*설명\\]?\\s*$\")),\n",
        "    (\"전시서문\", re.compile(r\"^\\s*\\[?전시\\s*서문\\]?\\s*$\")),\n",
        "    (\"작가노트\", re.compile(r\"^\\s*\\[?작가\\s*노트\\]?\\s*$\")),\n",
        "    (\"작가의 글\", re.compile(r\"^\\s*\\[?작가의\\s*글\\]?\\s*$\")),\n",
        "]\n",
        "\n",
        "def _extract_table_field(soup: BeautifulSoup, label_keywords: List[str]) -> Optional[str]:\n",
        "    for tr in soup.select(\"tr\"):\n",
        "        tds = tr.find_all(\"td\")\n",
        "        if len(tds) < 2:\n",
        "            continue\n",
        "        left = _clean_text(tds[0].get_text(\" \"))\n",
        "        right = _clean_text(tds[1].get_text(\" \"))\n",
        "        if not left:\n",
        "            left, right = right, left\n",
        "        if any(kw in left for kw in label_keywords):\n",
        "            if right:\n",
        "                return right\n",
        "    return None\n",
        "\n",
        "def _extract_title(soup: BeautifulSoup) -> Optional[str]:\n",
        "    t = _extract_table_field(soup, [\"제목\"])\n",
        "    if t:\n",
        "        return t\n",
        "    m = soup.find(\"meta\", attrs={\"property\": \"og:title\"})\n",
        "    if m and m.get(\"content\"):\n",
        "        t = _clean_text(m[\"content\"])\n",
        "        if t:\n",
        "            return t\n",
        "    candidates = []\n",
        "    for sel in [\"h1\", \"h2\", \".tit\", \".title\", \".subject\", \".board_tit\", \".view_tit\"]:\n",
        "        for el in soup.select(sel):\n",
        "            txt = _clean_text(el.get_text(\" \"))\n",
        "            if txt:\n",
        "                candidates.append((len(txt), txt))\n",
        "    if candidates:\n",
        "        candidates.sort(reverse=True)\n",
        "        return candidates[0][1]\n",
        "    if soup.title and soup.title.string:\n",
        "        return _clean_text(soup.title.string)\n",
        "    return None\n",
        "\n",
        "def _extract_period_from_table_or_text(soup: BeautifulSoup) -> Optional[str]:\n",
        "    p = _extract_table_field(soup, [\"기간\", \"전시기간\", \"전시 일정\", \"DATE\"])\n",
        "    if p:\n",
        "        return p\n",
        "    text = _clean_text(soup.get_text(\"\\n\"))\n",
        "    for pat in _PERIOD_PATTERNS:\n",
        "        m = pat.search(text)\n",
        "        if m:\n",
        "            s = _clean_text(m.group(\"s\")); e = _clean_text(m.group(\"e\"))\n",
        "            return f\"{s} - {e}\"\n",
        "    for line in text.splitlines():\n",
        "        if \"기간\" in line:\n",
        "            line = _clean_text(line)\n",
        "            if len(line) > 3:\n",
        "                return line\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [본문 스코프 & 푸터/노이즈 필터 헬퍼]\n",
        "_CONTENT_SELECTORS = \"#post_area, .board_view, .view, .view_area, .view_con, .board, #board, .content, .editor, .viewDetail\"\n",
        "_FOOTER_SELECTORS = \"footer, #footer, .footer, .foot, .ft, .bottom, .site-info, address\"\n",
        "\n",
        "_FOOTER_PAT = re.compile(\n",
        "    r\"(\"\n",
        "    r\"개인정보(처리|취급)방침|이메일무단수집거부|오시는길|고객센터|회사\\s*:|대표자|사업자|사업자등록|\"\n",
        "    r\"주소\\s*:|Tel\\s*:|Fax\\s*:|EMAIL\\s*:|E-?mail\\s*:|Copyright|COPYRIGHT|All\\s+Right[s]?\\s+Reserved\"\n",
        "    r\")\",\n",
        "    re.IGNORECASE\n",
        ")\n",
        "\n",
        "_UI_NOISE_SELECTORS = \"\"\"\n",
        "script, style, noscript,\n",
        "#footer, footer, .footer, .foot, .ft, .bottom, .site-info, address,\n",
        ".board_buttons, .board_btn, .post_btn, .post_buttons, .view_btns, .btn_area,\n",
        ".comment, .comments, #comments, #comment, .reply, #reply,\n",
        "#vote, .vote, .rating, .evaluate, .post_evaluate,\n",
        ".pagination, .pager, .navi, .nav,\n",
        ".sns_share, .share, .share_box,\n",
        ".prev, .next, .list, .btn, .bx-wrapper, .slider, .slide\n",
        "\"\"\".replace(\"\\n\", \" \")\n",
        "\n",
        "def get_content_scope(soup: BeautifulSoup) -> Tag:\n",
        "    scope = soup.select_one(_CONTENT_SELECTORS) or soup\n",
        "    for f in scope.select(_FOOTER_SELECTORS):\n",
        "        f.decompose()\n",
        "    return scope\n",
        "\n",
        "def is_footer_text(text: str) -> bool:\n",
        "    t = _clean_text(text)\n",
        "    if not t:\n",
        "        return False\n",
        "    return bool(_FOOTER_PAT.search(t))\n",
        "\n",
        "def trim_footer_tail(block_text: str) -> str:\n",
        "    if not block_text:\n",
        "        return block_text\n",
        "    lines = [l.rstrip() for l in block_text.splitlines()]\n",
        "    cut = len(lines)\n",
        "    for i, line in enumerate(lines):\n",
        "        if is_footer_text(line):\n",
        "            cut = i\n",
        "            break\n",
        "    cleaned = \"\\n\".join(lines[:cut]).rstrip()\n",
        "    cleaned = re.sub(r\"\\n{3,}\", \"\\n\\n\", cleaned)\n",
        "    return cleaned\n",
        "\n",
        "def prune_noise_nodes(root: Tag) -> None:\n",
        "    for n in root.select(_UI_NOISE_SELECTORS):\n",
        "        n.decompose()\n",
        "    for c in root.find_all(string=lambda t: isinstance(t, Comment)):\n",
        "        c.extract()\n",
        "    for el in root.find_all(True):\n",
        "        style = (el.get(\"style\") or \"\").lower()\n",
        "        if \"display:none\" in style or \"visibility:hidden\" in style:\n",
        "            el.decompose()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [섹션 추출 헬퍼]\n",
        "_LABEL_RE = re.compile(r\"^\\s*\\[?(전시\\s*설명|전시\\s*서문|작가\\s*노트|작가의\\s*글)\\]?\\s*$\")\n",
        "_LABEL_ANY_RE = re.compile(r\"\\[?\\s*(전시\\s*설명|전시\\s*서문|작가\\s*노트|작가의\\s*글)\\s*\\]?\")\n",
        "\n",
        "_HEADER_NOISE_PAT = re.compile(\n",
        "    r\"^\\s*(\"\n",
        "    r\"현재전시|제목\\s*:?\\s*|기간\\s*:?\\s*|장소\\s*:?\\s*|관람시간|관람료|문의|\"\n",
        "    r\"게시물\\s*평가|댓글\\s*쓰기|댓글\\s*목록|목록|이전\\s*다음|이전|다음|추천하기|수정하기|삭제하기|답글쓰기|글쓰기|\"\n",
        "    r\"슬라이드|게시판\\s*끝|컨텐츠\\s*끝|FOOTER|▼|▲|[-–—]\"\n",
        "    r\")\\s*$\",\n",
        "    re.IGNORECASE\n",
        ")\n",
        "\n",
        "def _is_label_text(t: str) -> bool:\n",
        "    return bool(_LABEL_RE.match(_clean_text(t or \"\")))\n",
        "\n",
        "def _strip_leading_until_label(text: str) -> str:\n",
        "    if not text:\n",
        "        return text\n",
        "    m = _LABEL_ANY_RE.search(text)\n",
        "    return text[m.end():].lstrip() if m else text\n",
        "\n",
        "_JS_TRASH_PAT = re.compile(r\"(function\\s+\\w+\\s*\\(|window\\.onload|document\\.getElementById|var\\s+\\w+\\s*=)\", re.I)\n",
        "def sanitize_text_block(text: str) -> str:\n",
        "    if not text:\n",
        "        return text\n",
        "    lines = []\n",
        "    for ln in text.splitlines():\n",
        "        t = ln.strip()\n",
        "        if t and (_JS_TRASH_PAT.search(t) or _HEADER_NOISE_PAT.match(t)):\n",
        "            continue\n",
        "        lines.append(ln)\n",
        "    out = \"\\n\".join(lines)\n",
        "    return re.sub(r\"\\n{3,}\", \"\\n\\n\", out).strip()\n",
        "\n",
        "def _nearest_block(tag: Tag) -> Tag:\n",
        "    cur = tag\n",
        "    while cur and isinstance(cur, Tag) and cur.name.lower() not in {\"p\", \"div\", \"li\", \"section\", \"article\"}:\n",
        "        cur = cur.parent\n",
        "    return cur if isinstance(cur, Tag) else tag\n",
        "\n",
        "def _collect_following_text(start: Tag) -> str:\n",
        "    buf: List[str] = []\n",
        "    it = start.next_elements\n",
        "    first = True\n",
        "    for el in it:\n",
        "        if first:\n",
        "            first = False\n",
        "            continue\n",
        "\n",
        "        if isinstance(el, Tag):\n",
        "            name = el.name.lower()\n",
        "            if name in {\"footer\", \"address\", \"hr\", \"h1\", \"h2\", \"h3\", \"h4\"}:\n",
        "                break\n",
        "            if el.select_one(_FOOTER_SELECTORS):\n",
        "                break\n",
        "\n",
        "        if isinstance(el, NavigableString):\n",
        "            txt = _clean_text(str(el))\n",
        "        elif isinstance(el, Tag):\n",
        "            txt = _clean_text(el.get_text(\"\\n\"))\n",
        "        else:\n",
        "            txt = \"\"\n",
        "\n",
        "        if not txt:\n",
        "            continue\n",
        "        if _is_label_text(txt):\n",
        "            break\n",
        "        if _HEADER_NOISE_PAT.match(txt.strip()):\n",
        "            continue\n",
        "\n",
        "        buf.append(txt)\n",
        "        if sum(len(x) for x in buf) > 12000:\n",
        "            break\n",
        "\n",
        "    out = \"\\n\".join(x for x in buf if x).strip()\n",
        "    out = trim_footer_tail(out)\n",
        "    out = _strip_leading_until_label(out)\n",
        "    out = sanitize_text_block(out)\n",
        "    return out\n",
        "\n",
        "def _extract_sections(soup: BeautifulSoup) -> Dict[str, str]:\n",
        "    root = get_content_scope(soup)\n",
        "    prune_noise_nodes(root)\n",
        "    textmap: Dict[str, str] = {}\n",
        "\n",
        "    for node in root.find_all(string=True):\n",
        "        s = _clean_text(str(node))\n",
        "        if not s:\n",
        "            continue\n",
        "        if _is_label_text(s):\n",
        "            block = _nearest_block(node if isinstance(node, Tag) else node.parent)\n",
        "            content = _collect_following_text(block)\n",
        "            if content:\n",
        "                label = _clean_text(s).strip(\"[]\")\n",
        "                textmap[label] = content\n",
        "\n",
        "    if not textmap:\n",
        "        for lab, pat in _SECTION_LABELS:\n",
        "            for el in root.find_all([\"h1\", \"h2\", \"h3\", \"strong\", \"b\", \"p\", \"div\"]):\n",
        "                t = _clean_text(el.get_text(\" \"))\n",
        "                if pat.match(t):\n",
        "                    content = _collect_following_text(el)\n",
        "                    if content:\n",
        "                        textmap[lab] = content\n",
        "\n",
        "    if not textmap:\n",
        "        paragraphs = [\n",
        "            _clean_text(p.get_text(\"\\n\"))\n",
        "            for p in root.find_all([\"p\", \"div\"])\n",
        "        ]\n",
        "\n",
        "        cleaned_paras = []\n",
        "        for para in paragraphs:\n",
        "            lines = [ln for ln in para.splitlines() if ln.strip()]\n",
        "            if lines and all(_HEADER_NOISE_PAT.match(ln) for ln in lines):\n",
        "                continue\n",
        "            cleaned_paras.append(para)\n",
        "\n",
        "        cleaned_paras = [\n",
        "            sanitize_text_block(trim_footer_tail(_strip_leading_until_label(x)))\n",
        "            for x in cleaned_paras if x and len(x) >= 40\n",
        "        ]\n",
        "        cleaned_paras.sort(key=len, reverse=True)\n",
        "        if cleaned_paras:\n",
        "            textmap[\"본문\"] = cleaned_paras[0]\n",
        "\n",
        "    return textmap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [이미지 수집 & 다운로드]\n",
        "_IMG_EXT = (\".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\", \".avif\")\n",
        "\n",
        "_EXCLUDE_PATH_SUBSTR = [\n",
        "    \"/default/img/common/\",\n",
        "    \"/img/common/\",\n",
        "    \"/component/board/board_10/list.gif\",\n",
        "    \"/component/board/board_10/write.gif\",\n",
        "]\n",
        "_EXCLUDE_NAME_EXACT = {\n",
        "    \"icon-phone.png\", \"icon-insta.png\", \"icon-blog.png\", \"icon-map.png\",\n",
        "    \"icon-top.png\", \"logo.png\", \"logo-m.png\", \"logo-f.png\",\n",
        "}\n",
        "_EXCLUDE_NAME_PREFIX = (\"icon\", \"logo\")\n",
        "\n",
        "def _norm_name_from_url(u: str) -> str:\n",
        "    name = os.path.basename(urlparse(u).path)\n",
        "    name = re.sub(r'^thumb-', '', name, flags=re.IGNORECASE)\n",
        "    name = re.sub(r'_(\\d+)x(\\d+)(?=\\.[A-Za-z0-9]+$)', '', name)\n",
        "    return name.lower()\n",
        "\n",
        "def dedupe_img_urls_by_key(img_urls: List[str]) -> List[str]:\n",
        "    uniq, seen = [], set()\n",
        "    for u in img_urls:\n",
        "        key = _norm_name_from_url(u)\n",
        "        if key and key not in seen:\n",
        "            seen.add(key)\n",
        "            uniq.append(u)\n",
        "    return uniq\n",
        "\n",
        "def _should_keep_image(u: str) -> bool:\n",
        "    p = urlparse(u).path.lower()\n",
        "    name = os.path.basename(p)\n",
        "    for sub in _EXCLUDE_PATH_SUBSTR:\n",
        "        if sub in p:\n",
        "            return False\n",
        "    if name in _EXCLUDE_NAME_EXACT:\n",
        "        return False\n",
        "    if name.startswith(_EXCLUDE_NAME_PREFIX):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def collect_image_urls(detail_url: str, soup: BeautifulSoup) -> List[str]:\n",
        "    urls: List[str] = []\n",
        "    scope = get_content_scope(soup)\n",
        "    for img in scope.find_all(\"img\"):\n",
        "        cand = None\n",
        "        for attr in (\"src\", \"data-src\", \"data-original\", \"data-lazy\", \"data-echo\"):\n",
        "            v = img.get(attr)\n",
        "            if v and isinstance(v, str):\n",
        "                cand = v\n",
        "                break\n",
        "        if not cand:\n",
        "            continue\n",
        "        u = _abs_url(detail_url, cand)\n",
        "        path = urlparse(u).path.lower()\n",
        "        if ((not os.path.splitext(path)[1]) or path.endswith(_IMG_EXT)) and _should_keep_image(u):\n",
        "            urls.append(u)\n",
        "    urls = dedupe_img_urls_by_key(urls)\n",
        "    return urls\n",
        "\n",
        "def _filename_from_url_or_headers(url: str, resp) -> str:\n",
        "    base = os.path.basename(urlparse(url).path)\n",
        "    if base:\n",
        "        return base\n",
        "    cd = resp.headers.get(\"Content-Disposition\", \"\")\n",
        "    m = re.search(r'filename\\*?=(?:UTF-8\\'\\')?\"?([^\";]+)\"?', cd)\n",
        "    if m:\n",
        "        return m.group(1)\n",
        "    ctype = (resp.headers.get(\"Content-Type\") or \"\").lower()\n",
        "    if \"png\" in ctype: return \"image.png\"\n",
        "    if \"webp\" in ctype: return \"image.webp\"\n",
        "    return \"image.jpg\"\n",
        "\n",
        "def download_images_from_urls(detail_url: str, img_urls: List[str], img_dir: str = IMG_DIR, max_imgs: Optional[int] = MAX_IMGS_PER_POST) -> List[str]:\n",
        "    if not img_urls:\n",
        "        return []\n",
        "    img_urls = dedupe_img_urls_by_key(img_urls)\n",
        "    qs = parse_qs(urlparse(detail_url).query)\n",
        "    post_id = qs.get(\"com_board_idx\", [\"unknown\"])[0]\n",
        "    subdir = os.path.join(img_dir, re.sub(r\"[^0-9A-Za-z_-]\", \"_\", post_id))\n",
        "    os.makedirs(subdir, exist_ok=True)\n",
        "\n",
        "    saved: List[str] = []\n",
        "    tried = 0\n",
        "    with get_session() as s:\n",
        "        _ = _get_soup(detail_url, s)\n",
        "        for u in img_urls:\n",
        "            if max_imgs is not None and tried >= max_imgs:\n",
        "                break\n",
        "            tried += 1\n",
        "            try:\n",
        "                r = s.get(\n",
        "                    u,\n",
        "                    headers={**HEADERS, \"Referer\": detail_url, \"Accept\": \"image/avif,image/webp,image/apng,image/*,*/*;q=0.8\"},\n",
        "                    timeout=TIMEOUT,\n",
        "                    allow_redirects=True,\n",
        "                )\n",
        "                ctype = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
        "                if r.status_code == 200 and r.content and \"image\" in ctype:\n",
        "                    name = _filename_from_url_or_headers(u, r)\n",
        "                    if \".\" not in os.path.basename(name):\n",
        "                        if \"png\" in ctype: name += \".png\"\n",
        "                        elif \"webp\" in ctype: name += \".webp\"\n",
        "                        else: name += \".jpg\"\n",
        "                    base, ext = os.path.splitext(name)\n",
        "                    final = os.path.join(subdir, name)\n",
        "                    k = 1\n",
        "                    while os.path.exists(final):\n",
        "                        final = os.path.join(subdir, f\"{base}_{k}{ext}\"); k += 1\n",
        "                    with open(final, \"wb\") as f:\n",
        "                        f.write(r.content)\n",
        "                    saved.append(final)\n",
        "                else:\n",
        "                    print(f\"[이미지 응답 이상] {r.status_code} {u} (ctype={ctype})\")\n",
        "            except Exception as e:\n",
        "                print(f\"[이미지 실패] {u} -> {e}\")\n",
        "    return saved\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [데이터 모델 & 상세 파서]\n",
        "@dataclass\n",
        "class ExhibitRecord:\n",
        "    url: str\n",
        "    title: str\n",
        "    period: str\n",
        "    section_type: str\n",
        "    section_text: str\n",
        "    image_urls: List[str]\n",
        "    saved_images: List[str]\n",
        "\n",
        "def parse_detail(url: str, s: Optional[requests.Session] = None, *, download_images: bool = DOWNLOAD_IMAGES) -> ExhibitRecord:\n",
        "    own = False\n",
        "    if s is None:\n",
        "        s = get_session(); own = True\n",
        "    try:\n",
        "        soup = _get_soup(url, s, referrer=LIST_URL)\n",
        "        title = _extract_title(soup) or \"\"\n",
        "        period = _extract_period_from_table_or_text(soup) or \"\"\n",
        "        sections = _extract_sections(soup)\n",
        "        order = [\"전시설명\", \"전시서문\", \"작가노트\", \"작가의 글\", \"본문\"]\n",
        "        section_type, section_text = \"\", \"\"\n",
        "        for k in order:\n",
        "            if k in sections and sections[k]:\n",
        "                section_type, section_text = k, sections[k]\n",
        "                break\n",
        "        image_urls = collect_image_urls(url, soup)\n",
        "        saved = download_images_from_urls(url, image_urls) if download_images else []\n",
        "        return ExhibitRecord(url=url, title=title, period=period, section_type=section_type, section_text=section_text, image_urls=image_urls, saved_images=saved)\n",
        "    finally:\n",
        "        if own:\n",
        "            s.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [엔드투엔드 크롤러]\n",
        "def crawl_maru_current(list_url: str = LIST_URL, *, max_pages: int = MAX_PAGES, limit: Optional[int] = None, download_images: bool = DOWNLOAD_IMAGES) -> List[ExhibitRecord]:\n",
        "    detail_urls = list_current_detail_urls(list_url, max_pages=max_pages)\n",
        "    if limit is not None:\n",
        "        detail_urls = detail_urls[:limit]\n",
        "    results: List[ExhibitRecord] = []\n",
        "    with get_session() as s:\n",
        "        _ = _get_soup(list_url, s)\n",
        "        for du in detail_urls:\n",
        "            try:\n",
        "                rec = parse_detail(du, s, download_images=download_images)\n",
        "                results.append(rec)\n",
        "            except Exception as e:\n",
        "                results.append(ExhibitRecord(url=du, title=\"\", period=\"\", section_type=\"\", section_text=f\"[ERROR] {e}\", image_urls=[], saved_images=[]))\n",
        "            time.sleep(SLEEP_BETWEEN)\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>period</th>\n",
              "      <th>section_type</th>\n",
              "      <th>images_count</th>\n",
              "      <th>first_image</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>제목</td>\n",
              "      <td>제목</td>\n",
              "      <td></td>\n",
              "      <td>5</td>\n",
              "      <td>https://maruartcenter.co.kr/bizdemo133414/comp...</td>\n",
              "      <td>https://maruartcenter.co.kr/default/exhibit/ex...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>제목</td>\n",
              "      <td>제목</td>\n",
              "      <td>전시설명</td>\n",
              "      <td>4</td>\n",
              "      <td>https://maruartcenter.co.kr/bizdemo133414/comp...</td>\n",
              "      <td>https://maruartcenter.co.kr/default/exhibit/ex...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>제목</td>\n",
              "      <td>제목</td>\n",
              "      <td>작가노트</td>\n",
              "      <td>3</td>\n",
              "      <td>https://maruartcenter.co.kr/bizdemo133414/comp...</td>\n",
              "      <td>https://maruartcenter.co.kr/default/exhibit/ex...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>제목</td>\n",
              "      <td>제목</td>\n",
              "      <td>전시서문</td>\n",
              "      <td>4</td>\n",
              "      <td>https://maruartcenter.co.kr/bizdemo133414/comp...</td>\n",
              "      <td>https://maruartcenter.co.kr/default/exhibit/ex...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>제목</td>\n",
              "      <td>제목</td>\n",
              "      <td></td>\n",
              "      <td>4</td>\n",
              "      <td>https://maruartcenter.co.kr/bizdemo133414/comp...</td>\n",
              "      <td>https://maruartcenter.co.kr/default/exhibit/ex...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>제목</td>\n",
              "      <td>제목</td>\n",
              "      <td></td>\n",
              "      <td>3</td>\n",
              "      <td>https://maruartcenter.co.kr/bizdemo133414/comp...</td>\n",
              "      <td>https://maruartcenter.co.kr/default/exhibit/ex...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  title period section_type  images_count  \\\n",
              "0    제목     제목                          5   \n",
              "1    제목     제목         전시설명             4   \n",
              "2    제목     제목         작가노트             3   \n",
              "3    제목     제목         전시서문             4   \n",
              "4    제목     제목                          4   \n",
              "5    제목     제목                          3   \n",
              "\n",
              "                                         first_image  \\\n",
              "0  https://maruartcenter.co.kr/bizdemo133414/comp...   \n",
              "1  https://maruartcenter.co.kr/bizdemo133414/comp...   \n",
              "2  https://maruartcenter.co.kr/bizdemo133414/comp...   \n",
              "3  https://maruartcenter.co.kr/bizdemo133414/comp...   \n",
              "4  https://maruartcenter.co.kr/bizdemo133414/comp...   \n",
              "5  https://maruartcenter.co.kr/bizdemo133414/comp...   \n",
              "\n",
              "                                                 url  \n",
              "0  https://maruartcenter.co.kr/default/exhibit/ex...  \n",
              "1  https://maruartcenter.co.kr/default/exhibit/ex...  \n",
              "2  https://maruartcenter.co.kr/default/exhibit/ex...  \n",
              "3  https://maruartcenter.co.kr/default/exhibit/ex...  \n",
              "4  https://maruartcenter.co.kr/default/exhibit/ex...  \n",
              "5  https://maruartcenter.co.kr/default/exhibit/ex...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved maru_current_exhibits.(csv|json)\n"
          ]
        }
      ],
      "source": [
        "# %% [실행 예시]\n",
        "DOWNLOAD_IMAGES = False\n",
        "MAX_PAGES = 1\n",
        "\n",
        "records = crawl_maru_current(LIST_URL, max_pages=MAX_PAGES, limit=10, download_images=DOWNLOAD_IMAGES)\n",
        "df = pd.DataFrame([{**asdict(r),\n",
        "                    \"images_count\": len(r.image_urls),\n",
        "                    \"first_image\": r.image_urls[0] if r.image_urls else \"\",\n",
        "                    \"saved_count\": len(r.saved_images)} for r in records])\n",
        "\n",
        "cols = [\"title\", \"period\", \"section_type\", \"images_count\", \"first_image\", \"url\"]\n",
        "display(df[cols])\n",
        "\n",
        "df.to_csv(\"maru_current_exhibits.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "df.to_json(\"maru_current_exhibits.json\", orient=\"records\", force_ascii=False)\n",
        "print(\"Saved maru_current_exhibits.(csv|json)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "442ee7aa",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8118bdb7",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
