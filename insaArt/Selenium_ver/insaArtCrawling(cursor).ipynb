{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdf62ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인사아트센터 현재 전시 크롤링\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from urllib.parse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7bb248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urljoin, urlparse\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# 필요한 라이브러리 설치 (필요시)\n",
    "# !pip install requests beautifulsoup4 pandas selenium pillow\n",
    "\n",
    "print(\"라이브러리 import 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510e317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인사아트센터 홈페이지 접속 및 구조 분석\n",
    "def get_page_content(url):\n",
    "    \"\"\"웹페이지 내용을 가져오는 함수\"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"페이지 요청 오류: {e}\")\n",
    "        return None\n",
    "\n",
    "# 인사아트센터 홈페이지 URL\n",
    "base_url = \"https://www.insaartcenter.com\"\n",
    "current_exhibitions_url = \"https://www.insaartcenter.com/exhibition/current\"\n",
    "\n",
    "print(f\"인사아트센터 홈페이지: {base_url}\")\n",
    "print(f\"현재 전시 페이지: {current_exhibitions_url}\")\n",
    "\n",
    "# 페이지 내용 확인\n",
    "page_content = get_page_content(current_exhibitions_url)\n",
    "if page_content:\n",
    "    print(\"페이지 로드 성공!\")\n",
    "    # HTML 구조 미리보기\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    print(\"\\\\n=== 페이지 제목 ===\")\n",
    "    print(soup.title.string if soup.title else \"제목 없음\")\n",
    "else:\n",
    "    print(\"페이지 로드 실패!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1014fa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML 구조 분석 및 전시 목록 추출\n",
    "def analyze_page_structure(soup):\n",
    "    \"\"\"페이지 구조를 분석하여 전시 정보가 있는 부분을 찾는 함수\"\"\"\n",
    "    print(\"\\\\n=== 페이지 구조 분석 ===\")\n",
    "    \n",
    "    # 가능한 전시 관련 클래스나 태그들 찾기\n",
    "    possible_selectors = [\n",
    "        'div[class*=\"exhibition\"]',\n",
    "        'div[class*=\"current\"]', \n",
    "        'div[class*=\"show\"]',\n",
    "        'article',\n",
    "        '.exhibition-item',\n",
    "        '.current-exhibition',\n",
    "        '.exhibition-list'\n",
    "    ]\n",
    "    \n",
    "    for selector in possible_selectors:\n",
    "        elements = soup.select(selector)\n",
    "        if elements:\n",
    "            print(f\"\\\\n발견된 요소: {selector} - {len(elements)}개\")\n",
    "            for i, elem in enumerate(elements[:3]):  # 처음 3개만 출력\n",
    "                print(f\"  [{i+1}] {elem.get_text(strip=True)[:100]}...\")\n",
    "    \n",
    "    # 링크들 분석\n",
    "    links = soup.find_all('a', href=True)\n",
    "    exhibition_links = [link for link in links if any(keyword in link.get('href', '').lower() \n",
    "                       for keyword in ['exhibition', 'show', 'current', '전시'])]\n",
    "    \n",
    "    print(f\"\\\\n전시 관련 링크 {len(exhibition_links)}개 발견:\")\n",
    "    for link in exhibition_links[:5]:  # 처음 5개만 출력\n",
    "        href = link.get('href')\n",
    "        text = link.get_text(strip=True)\n",
    "        print(f\"  - {text}: {href}\")\n",
    "\n",
    "if page_content:\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    analyze_page_structure(soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068b2de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전시 목록 크롤링 함수\n",
    "def extract_exhibition_list(soup, base_url):\n",
    "    \"\"\"전시 목록을 추출하는 함수\"\"\"\n",
    "    exhibitions = []\n",
    "    \n",
    "    # 다양한 선택자로 전시 정보 찾기\n",
    "    selectors_to_try = [\n",
    "        '.exhibition-item',\n",
    "        '.current-exhibition',\n",
    "        '.exhibition-card',\n",
    "        '.exhibition-list .item',\n",
    "        'article',\n",
    "        '.show-item',\n",
    "        'div[class*=\"exhibition\"]'\n",
    "    ]\n",
    "    \n",
    "    for selector in selectors_to_try:\n",
    "        items = soup.select(selector)\n",
    "        if items:\n",
    "            print(f\"\\\\n선택자 '{selector}'로 {len(items)}개 전시 발견\")\n",
    "            \n",
    "            for item in items:\n",
    "                exhibition_data = {}\n",
    "                \n",
    "                # 전시명 추출\n",
    "                title_selectors = ['h1', 'h2', 'h3', '.title', '.exhibition-title', 'a']\n",
    "                for title_sel in title_selectors:\n",
    "                    title_elem = item.select_one(title_sel)\n",
    "                    if title_elem and title_elem.get_text(strip=True):\n",
    "                        exhibition_data['title'] = title_elem.get_text(strip=True)\n",
    "                        break\n",
    "                \n",
    "                # 링크 추출\n",
    "                link_elem = item.find('a', href=True)\n",
    "                if link_elem:\n",
    "                    href = link_elem.get('href')\n",
    "                    if href:\n",
    "                        exhibition_data['link'] = urljoin(base_url, href)\n",
    "                \n",
    "                # 이미지 추출\n",
    "                img_elem = item.find('img')\n",
    "                if img_elem:\n",
    "                    img_src = img_elem.get('src') or img_elem.get('data-src')\n",
    "                    if img_src:\n",
    "                        exhibition_data['image_url'] = urljoin(base_url, img_src)\n",
    "                \n",
    "                # 전시 기간 추출\n",
    "                period_patterns = [\n",
    "                    r'\\\\d{4}\\\\.\\\\d{2}\\\\.\\\\d{2}.*?\\\\d{4}\\\\.\\\\d{2}\\\\.\\\\d{2}',\n",
    "                    r'\\\\d{4}\\\\.\\\\d{1,2}\\\\.\\\\d{1,2}.*?\\\\d{4}\\\\.\\\\d{1,2}\\\\.\\\\d{1,2}',\n",
    "                    r'\\\\d{4}-\\\\d{2}-\\\\d{2}.*?\\\\d{4}-\\\\d{2}-\\\\d{2}'\n",
    "                ]\n",
    "                \n",
    "                item_text = item.get_text()\n",
    "                for pattern in period_patterns:\n",
    "                    match = re.search(pattern, item_text)\n",
    "                    if match:\n",
    "                        exhibition_data['period'] = match.group()\n",
    "                        break\n",
    "                \n",
    "                # 작가명 추출 (보통 전시명 뒤에 나옴)\n",
    "                artist_patterns = [\n",
    "                    r'작가[:：]\\\\s*([^\\\\n]+)',\n",
    "                    r'Artist[:：]\\\\s*([^\\\\n]+)',\n",
    "                    r'Artist\\\\s*([^\\\\n]+)'\n",
    "                ]\n",
    "                \n",
    "                for pattern in artist_patterns:\n",
    "                    match = re.search(pattern, item_text, re.IGNORECASE)\n",
    "                    if match:\n",
    "                        exhibition_data['artist'] = match.group(1).strip()\n",
    "                        break\n",
    "                \n",
    "                # 전시장소 추출\n",
    "                venue_patterns = [\n",
    "                    r'장소[:：]\\\\s*([^\\\\n]+)',\n",
    "                    r'Venue[:：]\\\\s*([^\\\\n]+)',\n",
    "                    r'Location[:：]\\\\s*([^\\\\n]+)'\n",
    "                ]\n",
    "                \n",
    "                for pattern in venue_patterns:\n",
    "                    match = re.search(pattern, item_text, re.IGNORECASE)\n",
    "                    if match:\n",
    "                        exhibition_data['venue'] = match.group(1).strip()\n",
    "                        break\n",
    "                \n",
    "                # 유효한 데이터가 있는 경우만 추가\n",
    "                if exhibition_data.get('title') or exhibition_data.get('link'):\n",
    "                    exhibitions.append(exhibition_data)\n",
    "            \n",
    "            if exhibitions:\n",
    "                break\n",
    "    \n",
    "    return exhibitions\n",
    "\n",
    "# 전시 목록 추출 실행\n",
    "if page_content:\n",
    "    exhibitions = extract_exhibition_list(soup, base_url)\n",
    "    print(f\"\\\\n=== 추출된 전시 목록 ({len(exhibitions)}개) ===\")\n",
    "    for i, exhibition in enumerate(exhibitions, 1):\n",
    "        print(f\"\\\\n[{i}] 전시명: {exhibition.get('title', 'N/A')}\")\n",
    "        print(f\"    링크: {exhibition.get('link', 'N/A')}\")\n",
    "        print(f\"    기간: {exhibition.get('period', 'N/A')}\")\n",
    "        print(f\"    작가: {exhibition.get('artist', 'N/A')}\")\n",
    "        print(f\"    장소: {exhibition.get('venue', 'N/A')}\")\n",
    "        print(f\"    이미지: {exhibition.get('image_url', 'N/A')}\")\n",
    "else:\n",
    "    print(\"페이지 내용이 없어서 전시 목록을 추출할 수 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4d0fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개별 전시 상세 정보 크롤링 함수\n",
    "def extract_exhibition_details(exhibition_url):\n",
    "    \"\"\"개별 전시의 상세 정보를 추출하는 함수\"\"\"\n",
    "    if not exhibition_url:\n",
    "        return {}\n",
    "    \n",
    "    print(f\"\\\\n전시 상세 정보 크롤링: {exhibition_url}\")\n",
    "    \n",
    "    page_content = get_page_content(exhibition_url)\n",
    "    if not page_content:\n",
    "        return {}\n",
    "    \n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    details = {}\n",
    "    \n",
    "    # 전시 설명 추출\n",
    "    description_selectors = [\n",
    "        '.exhibition-description',\n",
    "        '.description',\n",
    "        '.content',\n",
    "        '.exhibition-content',\n",
    "        '.show-description',\n",
    "        'p',\n",
    "        '.text-content'\n",
    "    ]\n",
    "    \n",
    "    for selector in description_selectors:\n",
    "        desc_elem = soup.select_one(selector)\n",
    "        if desc_elem and desc_elem.get_text(strip=True):\n",
    "            details['description'] = desc_elem.get_text(strip=True)\n",
    "            break\n",
    "    \n",
    "    # 작품 이미지들 추출\n",
    "    image_selectors = [\n",
    "        '.exhibition-images img',\n",
    "        '.gallery img',\n",
    "        '.artwork-images img',\n",
    "        '.show-images img',\n",
    "        'img[class*=\"artwork\"]',\n",
    "        'img[class*=\"exhibition\"]'\n",
    "    ]\n",
    "    \n",
    "    artwork_images = []\n",
    "    for selector in image_selectors:\n",
    "        images = soup.select(selector)\n",
    "        for img in images:\n",
    "            img_src = img.get('src') or img.get('data-src')\n",
    "            if img_src:\n",
    "                full_url = urljoin(exhibition_url, img_src)\n",
    "                artwork_images.append(full_url)\n",
    "    \n",
    "    if artwork_images:\n",
    "        details['artwork_images'] = list(set(artwork_images))  # 중복 제거\n",
    "    \n",
    "    # 추가 정보 추출 (전시관, 관람시간 등)\n",
    "    info_selectors = [\n",
    "        '.exhibition-info',\n",
    "        '.show-info',\n",
    "        '.details',\n",
    "        '.info'\n",
    "    ]\n",
    "    \n",
    "    for selector in info_selectors:\n",
    "        info_elem = soup.select_one(selector)\n",
    "        if info_elem:\n",
    "            info_text = info_elem.get_text(strip=True)\n",
    "            details['additional_info'] = info_text\n",
    "            break\n",
    "    \n",
    "    return details\n",
    "\n",
    "# 전시 상세 정보 크롤링 실행\n",
    "if page_content and exhibitions:\n",
    "    print(\"\\\\n=== 전시 상세 정보 크롤링 시작 ===\")\n",
    "    \n",
    "    for i, exhibition in enumerate(exhibitions):\n",
    "        if exhibition.get('link'):\n",
    "            details = extract_exhibition_details(exhibition['link'])\n",
    "            \n",
    "            # 기본 정보에 상세 정보 추가\n",
    "            if details:\n",
    "                exhibition.update(details)\n",
    "            \n",
    "            print(f\"\\\\n[{i+1}] 전시 상세 정보 업데이트 완료\")\n",
    "            if details.get('description'):\n",
    "                print(f\"    설명: {details['description'][:100]}...\")\n",
    "            if details.get('artwork_images'):\n",
    "                print(f\"    작품 이미지: {len(details['artwork_images'])}개\")\n",
    "            \n",
    "            # 서버 부하 방지를 위한 대기\n",
    "            time.sleep(1)\n",
    "    \n",
    "    print(\"\\\\n전시 상세 정보 크롤링 완료!\")\n",
    "else:\n",
    "    print(\"전시 목록이 없어서 상세 정보를 크롤링할 수 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be64023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 작품 사진 다운로드 함수\n",
    "def download_image(image_url, save_path, headers=None):\n",
    "    \"\"\"이미지를 다운로드하는 함수\"\"\"\n",
    "    if not image_url:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        if headers is None:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "            }\n",
    "        \n",
    "        response = requests.get(image_url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # 파일 확장자 확인\n",
    "        parsed_url = urlparse(image_url)\n",
    "        file_extension = os.path.splitext(parsed_url.path)[1]\n",
    "        if not file_extension:\n",
    "            file_extension = '.jpg'  # 기본값\n",
    "        \n",
    "        # 파일명 생성 (URL에서 안전한 파일명 추출)\n",
    "        filename = os.path.basename(parsed_url.path)\n",
    "        if not filename or filename == '/':\n",
    "            filename = f\"image_{hash(image_url) % 10000}{file_extension}\"\n",
    "        \n",
    "        full_path = os.path.join(save_path, filename)\n",
    "        \n",
    "        with open(full_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        return full_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"이미지 다운로드 실패 ({image_url}): {e}\")\n",
    "        return False\n",
    "\n",
    "def download_exhibition_images(exhibitions, base_save_dir=\"exhibition_images\"):\n",
    "    \"\"\"전시 관련 이미지들을 다운로드하는 함수\"\"\"\n",
    "    if not os.path.exists(base_save_dir):\n",
    "        os.makedirs(base_save_dir)\n",
    "    \n",
    "    downloaded_images = {}\n",
    "    \n",
    "    for i, exhibition in enumerate(exhibitions):\n",
    "        exhibition_title = exhibition.get('title', f'exhibition_{i+1}')\n",
    "        # 파일명에 사용할 수 없는 문자 제거\n",
    "        safe_title = re.sub(r'[<>:\"/\\\\\\\\|?*]', '_', exhibition_title)\n",
    "        exhibition_dir = os.path.join(base_save_dir, safe_title)\n",
    "        \n",
    "        if not os.path.exists(exhibition_dir):\n",
    "            os.makedirs(exhibition_dir)\n",
    "        \n",
    "        exhibition_images = []\n",
    "        \n",
    "        # 메인 이미지 다운로드\n",
    "        if exhibition.get('image_url'):\n",
    "            main_image_path = download_image(exhibition['image_url'], exhibition_dir)\n",
    "            if main_image_path:\n",
    "                exhibition_images.append(main_image_path)\n",
    "                print(f\"메인 이미지 다운로드 완료: {main_image_path}\")\n",
    "        \n",
    "        # 작품 이미지들 다운로드\n",
    "        if exhibition.get('artwork_images'):\n",
    "            for j, artwork_url in enumerate(exhibition['artwork_images']):\n",
    "                artwork_path = download_image(artwork_url, exhibition_dir)\n",
    "                if artwork_path:\n",
    "                    exhibition_images.append(artwork_path)\n",
    "                    print(f\"작품 이미지 {j+1} 다운로드 완료: {artwork_path}\")\n",
    "                \n",
    "                # 서버 부하 방지\n",
    "                time.sleep(0.5)\n",
    "        \n",
    "        downloaded_images[exhibition_title] = exhibition_images\n",
    "    \n",
    "    return downloaded_images\n",
    "\n",
    "# 이미지 다운로드 실행 (옵션)\n",
    "print(\"\\\\n=== 작품 이미지 다운로드 ===\")\n",
    "print(\"이미지 다운로드를 시작하려면 아래 셀을 실행하세요.\")\n",
    "print(\"주의: 많은 이미지를 다운로드할 수 있으므로 시간이 오래 걸릴 수 있습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df54f89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 다운로드 실행 (실제 다운로드를 원할 경우 아래 주석 해제)\n",
    "# downloaded_images = download_exhibition_images(exhibitions)\n",
    "# print(f\"\\\\n총 {sum(len(images) for images in downloaded_images.values())}개의 이미지 다운로드 완료!\")\n",
    "\n",
    "# 현재는 다운로드하지 않고 URL만 표시\n",
    "print(\"\\\\n=== 이미지 URL 목록 ===\")\n",
    "for i, exhibition in enumerate(exhibitions, 1):\n",
    "    print(f\"\\\\n[{i}] {exhibition.get('title', 'N/A')}\")\n",
    "    if exhibition.get('image_url'):\n",
    "        print(f\"    메인 이미지: {exhibition['image_url']}\")\n",
    "    if exhibition.get('artwork_images'):\n",
    "        for j, img_url in enumerate(exhibition['artwork_images'], 1):\n",
    "            print(f\"    작품 이미지 {j}: {img_url}\")\n",
    "    else:\n",
    "        print(\"    작품 이미지: 없음\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b14d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 정리 및 저장\n",
    "def save_to_csv(exhibitions, filename=\"insa_art_center_exhibitions.csv\"):\n",
    "    \"\"\"전시 데이터를 CSV 파일로 저장\"\"\"\n",
    "    if not exhibitions:\n",
    "        print(\"저장할 전시 데이터가 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    # 데이터 정리\n",
    "    cleaned_data = []\n",
    "    for exhibition in exhibitions:\n",
    "        cleaned_item = {\n",
    "            '전시명': exhibition.get('title', ''),\n",
    "            '전시기간': exhibition.get('period', ''),\n",
    "            '작가': exhibition.get('artist', ''),\n",
    "            '전시장소': exhibition.get('venue', ''),\n",
    "            '전시관': exhibition.get('gallery', ''),\n",
    "            '전시설명': exhibition.get('description', ''),\n",
    "            '추가정보': exhibition.get('additional_info', ''),\n",
    "            '전시링크': exhibition.get('link', ''),\n",
    "            '메인이미지': exhibition.get('image_url', ''),\n",
    "            '작품이미지수': len(exhibition.get('artwork_images', [])),\n",
    "            '작품이미지URLs': '; '.join(exhibition.get('artwork_images', [])),\n",
    "            '크롤링일시': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        cleaned_data.append(cleaned_item)\n",
    "    \n",
    "    # DataFrame 생성 및 저장\n",
    "    df = pd.DataFrame(cleaned_data)\n",
    "    df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\\\n데이터가 '{filename}' 파일로 저장되었습니다.\")\n",
    "    print(f\"총 {len(cleaned_data)}개의 전시 정보가 저장되었습니다.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_to_json(exhibitions, filename=\"insa_art_center_exhibitions.json\"):\n",
    "    \"\"\"전시 데이터를 JSON 파일로 저장\"\"\"\n",
    "    if not exhibitions:\n",
    "        print(\"저장할 전시 데이터가 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    # 크롤링 시간 추가\n",
    "    data_with_metadata = {\n",
    "        'crawl_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'source': '인사아트센터',\n",
    "        'total_exhibitions': len(exhibitions),\n",
    "        'exhibitions': exhibitions\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data_with_metadata, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\\\n데이터가 '{filename}' 파일로 저장되었습니다.\")\n",
    "\n",
    "# 데이터 저장 실행\n",
    "if exhibitions:\n",
    "    print(\"\\\\n=== 데이터 저장 ===\")\n",
    "    \n",
    "    # CSV 저장\n",
    "    df = save_to_csv(exhibitions)\n",
    "    \n",
    "    # JSON 저장\n",
    "    save_to_json(exhibitions)\n",
    "    \n",
    "    # 요약 정보 출력\n",
    "    print(\"\\\\n=== 크롤링 결과 요약 ===\")\n",
    "    print(f\"총 전시 개수: {len(exhibitions)}\")\n",
    "    \n",
    "    for i, exhibition in enumerate(exhibitions, 1):\n",
    "        print(f\"\\\\n[{i}] {exhibition.get('title', 'N/A')}\")\n",
    "        print(f\"    기간: {exhibition.get('period', 'N/A')}\")\n",
    "        print(f\"    작가: {exhibition.get('artist', 'N/A')}\")\n",
    "        print(f\"    장소: {exhibition.get('venue', 'N/A')}\")\n",
    "        print(f\"    설명: {exhibition.get('description', 'N/A')[:100]}...\")\n",
    "        print(f\"    작품이미지: {len(exhibition.get('artwork_images', []))}개\")\n",
    "    \n",
    "    # DataFrame 미리보기\n",
    "    if df is not None:\n",
    "        print(\"\\\\n=== DataFrame 미리보기 ===\")\n",
    "        print(df.head())\n",
    "        \n",
    "else:\n",
    "    print(\"저장할 전시 데이터가 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2514d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 크롤링 프로세스 실행 함수\n",
    "def run_full_crawling():\n",
    "    \"\"\"전체 크롤링 프로세스를 실행하는 함수\"\"\"\n",
    "    print(\"=== 인사아트센터 전시 크롤링 시작 ===\")\n",
    "    \n",
    "    # 1. 페이지 로드\n",
    "    page_content = get_page_content(current_exhibitions_url)\n",
    "    if not page_content:\n",
    "        print(\"페이지 로드 실패!\")\n",
    "        return None\n",
    "    \n",
    "    # 2. 전시 목록 추출\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    exhibitions = extract_exhibition_list(soup, base_url)\n",
    "    \n",
    "    if not exhibitions:\n",
    "        print(\"전시 목록을 찾을 수 없습니다.\")\n",
    "        return None\n",
    "    \n",
    "    # 3. 상세 정보 크롤링\n",
    "    print(\"\\\\n전시 상세 정보 크롤링 중...\")\n",
    "    for i, exhibition in enumerate(exhibitions):\n",
    "        if exhibition.get('link'):\n",
    "            details = extract_exhibition_details(exhibition['link'])\n",
    "            if details:\n",
    "                exhibition.update(details)\n",
    "            time.sleep(1)  # 서버 부하 방지\n",
    "    \n",
    "    # 4. 데이터 저장\n",
    "    print(\"\\\\n데이터 저장 중...\")\n",
    "    df = save_to_csv(exhibitions)\n",
    "    save_to_json(exhibitions)\n",
    "    \n",
    "    print(\"\\\\n=== 크롤링 완료 ===\")\n",
    "    return exhibitions\n",
    "\n",
    "# 실행 예시 (필요시 주석 해제)\n",
    "# exhibitions = run_full_crawling()\n",
    "\n",
    "print(\"\\\\n크롤링 코드가 준비되었습니다!\")\n",
    "print(\"실행하려면 위의 'run_full_crawling()' 함수를 호출하세요.\")\n",
    "print(\"\\\\n주의사항:\")\n",
    "print(\"1. 웹사이트 구조가 변경될 수 있으므로 선택자를 조정해야 할 수 있습니다.\")\n",
    "print(\"2. 이미지 다운로드는 시간이 오래 걸릴 수 있습니다.\")\n",
    "print(\"3. 웹사이트의 robots.txt와 이용약관을 확인하세요.\")\n",
    "print(\"4. 과도한 요청으로 서버에 부하를 주지 않도록 주의하세요.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3416ae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
